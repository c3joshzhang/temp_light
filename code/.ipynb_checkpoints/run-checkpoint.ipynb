{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gurobipy as gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learn.info import ModelInfo\n",
    "from learn.feature import VarFeature, ConFeature, EdgFeature\n",
    "from learn.train import Inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "def maximum_independent_set_problem(\n",
    "    num_nodes=40,\n",
    "    edge_prob=0.3,\n",
    "    print_output=True\n",
    "):\n",
    "    edges = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(i+1, num_nodes):\n",
    "            if np.random.rand() < edge_prob:\n",
    "                edges.append((i, j))\n",
    "    m = gp.Model(\"maximum_independent_set\")\n",
    "    x = m.addVars(num_nodes, vtype=GRB.BINARY, name=\"x\")\n",
    "    for (i, j) in edges:\n",
    "        m.addConstr(x[i] + x[j] <= 1, name=f\"edge_{i}_{j}\")\n",
    "    m.setObjective(gp.quicksum(x[i] for i in range(num_nodes)), GRB.MAXIMIZE)\n",
    "    m.update()\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "var_features = []\n",
    "con_features = []\n",
    "edg_features = []\n",
    "solutions = []\n",
    "\n",
    "for i in range(2048):\n",
    "    m = maximum_independent_set_problem()\n",
    "    info = ModelInfo.from_model(m)\n",
    "    var_features.append(VarFeature.from_info(info.var_info, info.obj_info))\n",
    "    con_features.append(ConFeature.from_info(info.con_info))\n",
    "    edg_features.append(EdgFeature.from_info(info.con_info))\n",
    "    m.optimize()\n",
    "    s = [v.x for v in m.getVars()]\n",
    "    solutions.append(s)\n",
    "\n",
    "inst = Inst(var_features, con_features, edg_features, solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_v_edges, v_c_edges, node_features, edge_features, n_var, n_con = inst.xs\n",
    "ys = inst.ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_mask(size, ratio):\n",
    "    num_zero = int(round(size * ratio))\n",
    "    mask = torch.ones(size, dtype=torch.bool)\n",
    "    idx = torch.randperm(size)[:num_zero]\n",
    "    mask[idx] = 0\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_solution_mask(pool, ratio):\n",
    "    mask = pool.clone()\n",
    "    ones_indices = torch.where(mask == 1)[0]\n",
    "    num_keep = int(round(len(ones_indices) * ratio))\n",
    "    \n",
    "    if num_keep <= 0:\n",
    "        mask[ones_indices] = 0\n",
    "        return mask\n",
    "        \n",
    "    if num_keep >= len(ones_indices):\n",
    "        return mask\n",
    "\n",
    "    selected_indices = torch.randperm(len(ones_indices))[:num_keep]\n",
    "    keep_indices = ones_indices[selected_indices]\n",
    "    mask[ones_indices] = 0\n",
    "    mask[keep_indices] = 1\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_mask_node_feature(node_feature, y, mask):\n",
    "    node_feature_with_y = torch.hstack([node_feature, y.unsqueeze(1)])\n",
    "    mask = torch.cat([mask, torch.zeros(len(y) - len(mask), dtype=torch.bool)])\n",
    "    masked = node_feature_with_y.clone()\n",
    "    masked[mask, -1] = 0\n",
    "    return torch.hstack([masked, mask.unsqueeze(1)]), mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from learn.model import FocalLoss, SpGAT\n",
    "import torch.optim as optim\n",
    "\n",
    "model = SpGAT(\n",
    "    nfeat=inst.xs[2][0].shape[1] + 2,\n",
    "    nhid=64,\n",
    "    nclass=2,\n",
    "    dropout=0.1,\n",
    "    nheads=6,\n",
    "    alpha=0.2\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "ce = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.5\n",
    "train_sets = []\n",
    "for i in range(len(ys)):\n",
    "    n = n_var[i]\n",
    "    s = get_train_mask(n, train_ratio)\n",
    "    train_sets.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6504122  0.34958786]\n",
      " [0.6453836  0.3546164 ]\n",
      " [0.63341224 0.3665877 ]\n",
      " [0.644787   0.35521305]\n",
      " [0.64602137 0.3539786 ]\n",
      " [0.6251017  0.37489834]\n",
      " [0.65250975 0.34749028]\n",
      " [0.6371693  0.36283073]\n",
      " [0.65343887 0.34656104]\n",
      " [0.6248499  0.37515002]\n",
      " [0.65365356 0.34634644]\n",
      " [0.6455813  0.3544187 ]\n",
      " [0.6511008  0.34889916]\n",
      " [0.64518285 0.35481712]\n",
      " [0.64506537 0.35493463]\n",
      " [0.63329595 0.36670405]\n",
      " [0.6248754  0.37512448]\n",
      " [0.6406751  0.3593249 ]\n",
      " [0.6542827  0.3457173 ]\n",
      " [0.6497885  0.35021153]] [0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0] tensor(10)\n",
      "tensor(36.6236, grad_fn=<AddBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[0.7451815  0.2548185 ]\n",
      " [0.6497897  0.35021037]\n",
      " [0.75967807 0.24032192]\n",
      " [0.73720044 0.2627996 ]\n",
      " [0.7516497  0.24835034]\n",
      " [0.6169149  0.38308507]\n",
      " [0.72300595 0.27699405]\n",
      " [0.7585104  0.24148957]\n",
      " [0.7479056  0.25209445]\n",
      " [0.7476415  0.25235844]\n",
      " [0.6519726  0.34802738]\n",
      " [0.65970355 0.34029642]\n",
      " [0.61718714 0.38281283]\n",
      " [0.7632966  0.23670349]\n",
      " [0.7472177  0.25278232]\n",
      " [0.60942    0.39057997]\n",
      " [0.73076785 0.26923215]\n",
      " [0.73213935 0.26786068]\n",
      " [0.71974105 0.28025898]\n",
      " [0.7427755  0.25722444]] [0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 0 0 0 0] tensor(10)\n",
      "tensor(31.6649, grad_fn=<AddBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[0.58384335 0.41615668]\n",
      " [0.8243441  0.17565589]\n",
      " [0.5745984  0.42540166]\n",
      " [0.60453933 0.39546075]\n",
      " [0.79711014 0.20288983]\n",
      " [0.77788115 0.2221189 ]\n",
      " [0.78603834 0.2139617 ]\n",
      " [0.8141779  0.1858221 ]\n",
      " [0.7779982  0.22200179]\n",
      " [0.6309594  0.36904052]\n",
      " [0.8274901  0.17250992]\n",
      " [0.8070978  0.19290224]\n",
      " [0.79434085 0.20565912]\n",
      " [0.568886   0.43111405]\n",
      " [0.797909   0.20209101]\n",
      " [0.76109517 0.23890485]\n",
      " [0.60743606 0.3925639 ]\n",
      " [0.8309388  0.16906115]\n",
      " [0.56975543 0.43024454]\n",
      " [0.81033194 0.18966809]] [1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0] tensor(10)\n",
      "tensor(26.4787, grad_fn=<AddBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[0.75217474 0.2478252 ]\n",
      " [0.72514796 0.27485204]\n",
      " [0.74145377 0.25854623]\n",
      " [0.7172407  0.2827593 ]\n",
      " [0.7273566  0.27264342]\n",
      " [0.76432323 0.23567681]\n",
      " [0.7795891  0.2204109 ]\n",
      " [0.40511933 0.59488064]\n",
      " [0.79874295 0.20125706]\n",
      " [0.7682159  0.23178408]\n",
      " [0.72832406 0.27167588]\n",
      " [0.71415806 0.28584185]\n",
      " [0.72923225 0.27076778]\n",
      " [0.79155034 0.20844968]\n",
      " [0.38531768 0.6146823 ]\n",
      " [0.7322711  0.26772892]\n",
      " [0.68108296 0.31891707]\n",
      " [0.6903065  0.3096935 ]\n",
      " [0.70601994 0.29398003]\n",
      " [0.7007533  0.29924676]] [0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0] tensor(10)\n",
      "tensor(20.8299, grad_fn=<AddBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[0.7958203  0.20417973]\n",
      " [0.78596663 0.2140333 ]\n",
      " [0.80213004 0.19786994]\n",
      " [0.740567   0.25943297]\n",
      " [0.7685005  0.2314995 ]\n",
      " [0.7231185  0.27688152]\n",
      " [0.73584235 0.26415765]\n",
      " [0.22715527 0.77284473]\n",
      " [0.8025507  0.19744931]\n",
      " [0.29300416 0.7069959 ]\n",
      " [0.81571317 0.18428683]\n",
      " [0.82005155 0.17994851]\n",
      " [0.2379569  0.7620431 ]\n",
      " [0.29703617 0.7029638 ]\n",
      " [0.7724691  0.2275309 ]\n",
      " [0.7020176  0.29798234]\n",
      " [0.73757064 0.26242933]\n",
      " [0.76106554 0.23893452]\n",
      " [0.7310516  0.26894838]\n",
      " [0.24707942 0.75292057]] [0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1] tensor(10)\n",
      "tensor(17.2638, grad_fn=<AddBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[0.80472755 0.19527248]\n",
      " [0.89772546 0.10227451]\n",
      " [0.3434623  0.6565378 ]\n",
      " [0.8731702  0.12682983]\n",
      " [0.905524   0.09447594]\n",
      " [0.8950866  0.1049135 ]\n",
      " [0.23988013 0.76011986]\n",
      " [0.29199615 0.7080038 ]\n",
      " [0.91651684 0.08348316]\n",
      " [0.85912174 0.14087832]\n",
      " [0.9090071  0.09099299]\n",
      " [0.9128588  0.08714128]\n",
      " [0.34800756 0.65199244]\n",
      " [0.23827368 0.76172626]\n",
      " [0.31409544 0.6859046 ]\n",
      " [0.93328506 0.06671497]\n",
      " [0.92439854 0.07560145]\n",
      " [0.8949537  0.10504631]\n",
      " [0.8773553  0.12264471]\n",
      " [0.4368415  0.56315845]] [0 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1] tensor(10)\n",
      "tensor(12.5553, grad_fn=<AddBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[0.98214555 0.01785443]\n",
      " [0.34776285 0.6522372 ]\n",
      " [0.3686452  0.6313548 ]\n",
      " [0.98209506 0.01790497]\n",
      " [0.98235875 0.01764128]\n",
      " [0.9792437  0.02075627]\n",
      " [0.30070874 0.6992912 ]\n",
      " [0.97557586 0.0244242 ]\n",
      " [0.36232817 0.6376719 ]\n",
      " [0.45277575 0.5472243 ]\n",
      " [0.98377323 0.01622676]\n",
      " [0.98155653 0.0184435 ]\n",
      " [0.9769404  0.02305964]\n",
      " [0.97402394 0.02597607]\n",
      " [0.51634103 0.48365888]\n",
      " [0.9820749  0.01792505]\n",
      " [0.9793773  0.02062265]\n",
      " [0.97847956 0.0215205 ]\n",
      " [0.9850395  0.01496049]\n",
      " [0.9860115  0.01398848]] [0 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0] tensor(10)\n",
      "tensor(13.9773, grad_fn=<AddBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[0.9894918  0.01050815]\n",
      " [0.98464465 0.01535541]\n",
      " [0.98326844 0.01673154]\n",
      " [0.98345196 0.01654811]\n",
      " [0.9771593  0.02284068]\n",
      " [0.98304486 0.01695517]\n",
      " [0.23323879 0.7667612 ]\n",
      " [0.9890708  0.01092917]\n",
      " [0.985386   0.01461402]\n",
      " [0.17840444 0.82159555]\n",
      " [0.1888081  0.81119186]\n",
      " [0.97005314 0.02994688]\n",
      " [0.3297387  0.6702613 ]\n",
      " [0.98775035 0.01224967]\n",
      " [0.986961   0.01303894]\n",
      " [0.1408882  0.8591118 ]\n",
      " [0.14353415 0.8564658 ]\n",
      " [0.98719794 0.01280205]\n",
      " [0.99004996 0.00995007]\n",
      " [0.98391956 0.01608038]] [0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0] tensor(10)\n",
      "tensor(8.7763, grad_fn=<AddBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[0.9645168  0.03548326]\n",
      " [0.94419223 0.05580775]\n",
      " [0.9400884  0.05991162]\n",
      " [0.98634803 0.01365196]\n",
      " [0.04449229 0.95550776]\n",
      " [0.03082771 0.9691723 ]\n",
      " [0.9542845  0.04571556]\n",
      " [0.9445654  0.05543461]\n",
      " [0.9677385  0.03226151]\n",
      " [0.97411615 0.02588391]\n",
      " [0.9075491  0.09245092]\n",
      " [0.97841567 0.02158436]\n",
      " [0.05135521 0.9486448 ]\n",
      " [0.9630061  0.03699398]\n",
      " [0.9582474  0.04175258]\n",
      " [0.9485992  0.05140082]\n",
      " [0.9447846  0.0552154 ]\n",
      " [0.97020817 0.0297918 ]\n",
      " [0.97679216 0.02320786]\n",
      " [0.9412263  0.05877371]] [0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0] tensor(10)\n",
      "tensor(7.2231, grad_fn=<AddBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[0.01427371 0.98572636]\n",
      " [0.9163712  0.08362877]\n",
      " [0.9232812  0.07671875]\n",
      " [0.9538027  0.04619727]\n",
      " [0.96482617 0.03517377]\n",
      " [0.96320254 0.03679748]\n",
      " [0.91232026 0.08767977]\n",
      " [0.95259506 0.04740503]\n",
      " [0.9379854  0.06201462]\n",
      " [0.9445177  0.05548237]\n",
      " [0.9249009  0.07509906]\n",
      " [0.01804401 0.98195595]\n",
      " [0.9247852  0.0752148 ]\n",
      " [0.9593526  0.04064739]\n",
      " [0.8927608  0.10723926]\n",
      " [0.01638066 0.9836194 ]\n",
      " [0.8944762  0.1055238 ]\n",
      " [0.9091868  0.0908132 ]\n",
      " [0.97194463 0.02805544]\n",
      " [0.965995   0.03400493]] [1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0] tensor(10)\n",
      "tensor(8.2139, grad_fn=<AddBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[0.9719605  0.02803948]\n",
      " [0.01745723 0.98254275]\n",
      " [0.979511   0.0204889 ]\n",
      " [0.97386116 0.02613891]\n",
      " [0.98264194 0.01735801]\n",
      " [0.9851596  0.01484045]\n",
      " [0.00936325 0.99063677]\n",
      " [0.01065007 0.9893499 ]\n",
      " [0.9881202  0.01187983]\n",
      " [0.9876474  0.01235253]\n",
      " [0.961949   0.03805096]\n",
      " [0.9895872  0.01041281]\n",
      " [0.98731625 0.01268378]\n",
      " [0.95408607 0.04591399]\n",
      " [0.01350314 0.98649687]\n",
      " [0.98787284 0.01212721]\n",
      " [0.9697906  0.03020939]\n",
      " [0.01250613 0.9874938 ]\n",
      " [0.01001428 0.9899857 ]\n",
      " [0.9806789  0.01932101]] [0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0] tensor(10)\n",
      "tensor(6.4390, grad_fn=<AddBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[0.99596393 0.00403608]\n",
      " [0.05163359 0.9483664 ]\n",
      " [0.02935221 0.9706478 ]\n",
      " [0.02266683 0.9773332 ]\n",
      " [0.9981079  0.00189209]\n",
      " [0.9966667  0.00333329]\n",
      " [0.02605626 0.97394377]\n",
      " [0.9975823  0.00241766]\n",
      " [0.99707294 0.00292707]\n",
      " [0.9973283  0.00267174]\n",
      " [0.03542461 0.96457535]\n",
      " [0.9964418  0.00355816]\n",
      " [0.99364704 0.00635296]\n",
      " [0.9949826  0.00501739]\n",
      " [0.04938537 0.9506147 ]\n",
      " [0.9980774  0.00192264]\n",
      " [0.04041509 0.95958495]\n",
      " [0.99431473 0.0056853 ]\n",
      " [0.9964671  0.00353287]\n",
      " [0.02543777 0.9745623 ]] [0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1] tensor(10)\n",
      "tensor(6.1299, grad_fn=<AddBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[9.99959111e-01 4.08369779e-05]\n",
      " [9.99899745e-01 1.00229889e-04]\n",
      " [9.99917865e-01 8.21580834e-05]\n",
      " [7.05356717e-01 2.94643253e-01]\n",
      " [2.07606971e-01 7.92393029e-01]\n",
      " [9.99802053e-01 1.97917558e-04]\n",
      " [9.99793231e-01 2.06766985e-04]\n",
      " [3.48628312e-01 6.51371658e-01]\n",
      " [9.99898076e-01 1.01937512e-04]\n",
      " [9.99889851e-01 1.10152469e-04]\n",
      " [9.99943018e-01 5.69803524e-05]\n",
      " [9.99881148e-01 1.18864795e-04]\n",
      " [5.88748932e-01 4.11251038e-01]\n",
      " [7.06884623e-01 2.93115407e-01]\n",
      " [9.99945760e-01 5.42741000e-05]\n",
      " [9.99895811e-01 1.04214625e-04]\n",
      " [9.99792516e-01 2.07525620e-04]\n",
      " [9.99676585e-01 3.23372340e-04]\n",
      " [2.76405811e-01 7.23594189e-01]\n",
      " [9.99946117e-01 5.38731729e-05]] [0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0] tensor(10)\n",
      "tensor(14.5447, grad_fn=<AddBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[9.9998724e-01 1.2762377e-05]\n",
      " [9.9996614e-01 3.3807948e-05]\n",
      " [9.9998307e-01 1.6868356e-05]\n",
      " [9.9998629e-01 1.3719455e-05]\n",
      " [9.9998951e-01 1.0514331e-05]\n",
      " [9.9998510e-01 1.4910386e-05]\n",
      " [9.9998903e-01 1.0970498e-05]\n",
      " [8.5697728e-01 1.4302275e-01]\n",
      " [9.9997747e-01 2.2484894e-05]\n",
      " [9.9998605e-01 1.3916563e-05]\n",
      " [9.9997842e-01 2.1544200e-05]\n",
      " [7.2749764e-01 2.7250236e-01]\n",
      " [8.6402929e-01 1.3597077e-01]\n",
      " [9.9997592e-01 2.4058405e-05]\n",
      " [9.9998653e-01 1.3429359e-05]\n",
      " [6.7747825e-01 3.2252181e-01]\n",
      " [9.9998319e-01 1.6848502e-05]\n",
      " [6.0962075e-01 3.9037928e-01]\n",
      " [9.9997842e-01 2.1518164e-05]\n",
      " [9.9997580e-01 2.4152263e-05]] [0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0] tensor(10)\n",
      "tensor(28.7613, grad_fn=<AddBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[9.99985099e-01 1.48815052e-05]\n",
      " [9.99991655e-01 8.29498549e-06]\n",
      " [9.99988794e-01 1.12198195e-05]\n",
      " [9.99963403e-01 3.65991000e-05]\n",
      " [9.99982119e-01 1.79315284e-05]\n",
      " [9.99993086e-01 6.96931147e-06]\n",
      " [9.99991059e-01 8.97222071e-06]\n",
      " [9.99991059e-01 8.90595584e-06]\n",
      " [9.99984622e-01 1.53636283e-05]\n",
      " [6.83301508e-01 3.16698521e-01]\n",
      " [9.99991417e-01 8.62481011e-06]\n",
      " [9.99992847e-01 7.10971608e-06]\n",
      " [9.99992847e-01 7.12345945e-06]\n",
      " [7.04548776e-01 2.95451194e-01]\n",
      " [9.99990106e-01 9.88277952e-06]\n",
      " [9.99990344e-01 9.62112790e-06]\n",
      " [6.09028459e-01 3.90971512e-01]\n",
      " [9.99983191e-01 1.67684339e-05]\n",
      " [9.99987006e-01 1.29353066e-05]\n",
      " [5.77542841e-01 4.22457188e-01]] [0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1] tensor(10)\n",
      "tensor(18.0794, grad_fn=<AddBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "batch_size = 256\n",
    "for epoch in range(500):\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    agg_loss = 0\n",
    "    inst_idxs = list(range(len(ys)))\n",
    "    random.shuffle(inst_idxs)\n",
    "\n",
    "    counter = 0\n",
    "    for i in inst_idxs:\n",
    "        counter += 1        \n",
    "\n",
    "        t_mask = train_sets[i]\n",
    "        s_mask = get_solution_mask(t_mask, 0.5)\n",
    "        nf, mask = get_mask_node_feature(node_features[i], ys[i], s_mask)\n",
    "        \n",
    "        output, _ = model(\n",
    "            nf, \n",
    "            c_v_edges[i], \n",
    "            v_c_edges[i], \n",
    "            edge_features[i].detach()\n",
    "        )\n",
    "        fl = FocalLoss()\n",
    "\n",
    "        train_mask = torch.cat([~t_mask, torch.zeros(len(mask) - len(t_mask), dtype=torch.bool)])\n",
    "        loss = fl(output[train_mask, :], ys[i][train_mask])\n",
    "        agg_loss += loss\n",
    "        \n",
    "        if counter >= batch_size:\n",
    "            (agg_loss/batch_size).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "            print(output[train_mask, :].detach().numpy(), ys[i][train_mask].detach().numpy(), mask.sum())\n",
    "            print(agg_loss)\n",
    "            print('-'*100)\n",
    "            agg_loss = 0\n",
    "            counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys[i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1 == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst = Inst(\n",
    "    [VarFeature.from_info(info.var_info, info.obj_info)],\n",
    "    [ConFeature.from_info(info.con_info)],\n",
    "    [EdgFeature.from_info(info.con_info)],\n",
    "    [[v.x for v in m.getVars()]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    agg_loss = 0\n",
    "    for i in range(5):\n",
    "        inst_idx = random.randint(0, len(ys) - 1)\n",
    "        train_idx = torch.as_tensor(range(2), dtype=torch.int32)\n",
    "        \n",
    "        output, edge_features[inst_idx] = model(\n",
    "            node_features[inst_idx], \n",
    "            c_v_edges[inst_idx], \n",
    "            v_c_edges[inst_idx], \n",
    "            edge_features[inst_idx].detach()\n",
    "        )\n",
    "        fl = FocalLoss()\n",
    "        loss = fl(output[train_idx], ys[inst_idx][train_idx])\n",
    "        agg_loss += loss\n",
    "        \n",
    "    print(output[train_idx].detach().numpy(), ys[inst_idx][train_idx].detach().numpy())\n",
    "    print(agg_loss)\n",
    "    \n",
    "    agg_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1 == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, num):\n",
    "    global data_edge_features\n",
    "    t = time.time()\n",
    "\n",
    "    output, data_edge_features[num] = model(data_features[num], data_edge_A[num], data_edge_B[num], data_edge_features[num].detach())\n",
    "    print(data_solution[num][idx_train])\n",
    "\n",
    "    lf = Focal_Loss(torch.as_tensor(data_labels[num]))\n",
    "    loss_train = lf(output[idx_train], data_solution[num][idx_train])\n",
    "\n",
    "    return loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = time.time()\n",
    "loss_values = []\n",
    "for epoch in range(args.epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    now_loss = 0\n",
    "    \n",
    "    for i in range(5):\n",
    "        now_data = random.randint(0, data_num - 1)\n",
    "        now_loss += train(epoch, now_data)\n",
    "        \n",
    "    loss_values.append(now_loss)\n",
    "    now_loss.backward()\n",
    "    optimizer.step()\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(now_loss))\n",
    "\n",
    "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
    "    if loss_values[-1] < best:\n",
    "        best = loss_values[-1]\n",
    "        best_epoch = epoch\n",
    "        bad_counter = 0\n",
    "    else:\n",
    "        bad_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "model = SpGAT(nfeat=data_features[0].shape[1],    # Feature dimension\n",
    "            nhid=args.hidden,             # Feature dimension of each hidden layer\n",
    "            nclass=int(data_solution[0].max()) + 1, # Number of classes\n",
    "            dropout=args.dropout,         # Dropout\n",
    "            nheads=args.nb_heads,         # Number of heads\n",
    "            alpha=args.alpha)             # LeakyReLU alpha coefficient\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),    \n",
    "                       lr=args.lr,                        # Learning rate\n",
    "                       weight_decay=args.weight_decay)    # Weight decay to prevent overfitting\n",
    "\n",
    "if args.cuda: # Move to GPU\n",
    "    model.to(device)\n",
    "    for now_data in range(data_num):\n",
    "        data_features[now_data] = data_features[now_data].to(device)\n",
    "        data_labels[now_data] = data_labels[now_data].to(device)\n",
    "        data_solution[now_data] = data_solution[now_data].to(device)\n",
    "        data_edge_A[now_data] = data_edge_A[now_data].to(device)\n",
    "        data_edge_B[now_data] = data_edge_B[now_data].to(device)\n",
    "        data_edge_features[now_data] = data_edge_features[now_data].to(device)\n",
    "        data_idx_train[now_data] = data_idx_train[now_data].to(device)\n",
    "\n",
    "\n",
    "for now_data in range(data_num):\n",
    "    data_features[now_data] = Variable(data_features[now_data])\n",
    "    data_edge_A[now_data] = Variable(data_edge_A[now_data])\n",
    "    data_edge_B[now_data] = Variable(data_edge_B[now_data])\n",
    "    data_solution[now_data] = Variable(data_solution[now_data])\n",
    "    # Define computation graph for automatic differentiation\n",
    "\n",
    "def train(epoch, num):\n",
    "    global data_edge_features\n",
    "    t = time.time()\n",
    "\n",
    "    output, data_edge_features[num] = model(data_features[num], data_edge_A[num], data_edge_B[num], data_edge_features[num].detach())\n",
    "    print(data_solution[num][idx_train])\n",
    "\n",
    "    lf = Focal_Loss(torch.as_tensor(data_labels[num]))\n",
    "    loss_train = lf(output[idx_train], data_solution[num][idx_train])\n",
    "\n",
    "    return loss_train\n",
    "\n",
    "t_total = time.time()\n",
    "loss_values = []\n",
    "bad_counter = 0\n",
    "best = args.epochs + 1\n",
    "best_epoch = 0\n",
    "for epoch in range(args.epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    now_loss = 0\n",
    "    for i in range(5):\n",
    "        now_data = random.randint(0, data_num - 1)\n",
    "        now_loss += train(epoch, now_data)\n",
    "    loss_values.append(now_loss)\n",
    "    now_loss.backward()\n",
    "    optimizer.step()\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(now_loss))\n",
    "\n",
    "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
    "    if loss_values[-1] < best:\n",
    "        best = loss_values[-1]\n",
    "        best_epoch = epoch\n",
    "        bad_counter = 0\n",
    "    else:\n",
    "        bad_counter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
