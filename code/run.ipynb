{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gurobipy as gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[x86] - Darwin 22.4.0 22E252)\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Optimize a model with 1 rows, 2 columns and 2 nonzeros\n",
      "Model fingerprint: 0x068e17f7\n",
      "Variable types: 0 continuous, 2 integer (2 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [0e+00, 0e+00]\n",
      "Found heuristic solution: objective 2.0000000\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.01 seconds (0.00 work units)\n",
      "Thread count was 1 (of 16 available processors)\n",
      "\n",
      "Solution count 1: 2 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 2.000000000000e+00, best bound 2.000000000000e+00, gap 0.0000%\n"
     ]
    }
   ],
   "source": [
    "m = gp.Model()\n",
    "v1 = m.addVar(vtype=gp.GRB.BINARY)\n",
    "v2 = m.addVar(vtype=gp.GRB.BINARY)\n",
    "m.addConstr(v1 >= v2)\n",
    "m.setObjective(v1 + v2, sense=gp.GRB.MAXIMIZE)\n",
    "m.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learn.info import ModelInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = ModelInfo.from_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 1.0, 'B'), (0.0, 1.0, 'B')]\n",
       "[([0, 1], [1.0, -1.0], '>=', 0.0)]\n",
       "[{0: 1.0, 1: 1.0}, -1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learn.feature import VarFeature, ConFeature, EdgFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learn.train import Inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst = Inst(\n",
    "    [VarFeature.from_info(info.var_info, info.obj_info)],\n",
    "    [ConFeature.from_info(info.con_info)],\n",
    "    [EdgFeature.from_info(info.con_info)],\n",
    "    [[v.x for v in m.getVars()]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from learn.model import FocalLoss, SpGAT\n",
    "import torch.optim as optim\n",
    "\n",
    "model = SpGAT(\n",
    "    nfeat=inst.xs[2][0].shape[1],\n",
    "    nhid=64,\n",
    "    nclass=2,\n",
    "    dropout=0.5,\n",
    "    nheads=6,\n",
    "    alpha=0.2\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2, weight_decay=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_v_edges, v_c_edges, node_features, edge_features = inst.xs\n",
    "ys = inst.ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5488219 0.451178 ]\n",
      " [0.5580024 0.4419976]] [1 1]\n",
      "tensor(1.0384, grad_fn=<AddBackward0>)\n",
      "[[0.00244018 0.9975598 ]\n",
      " [0.00242765 0.99757236]] [1 1]\n",
      "tensor(0.1146, grad_fn=<AddBackward0>)\n",
      "[[1.0370454e-10 1.0000000e+00]\n",
      " [1.0645256e-10 1.0000000e+00]] [1 1]\n",
      "tensor(0.1133, grad_fn=<AddBackward0>)\n",
      "[[3.8930806e-20 1.0000000e+00]\n",
      " [4.0239603e-20 1.0000000e+00]] [1 1]\n",
      "tensor(0.1133, grad_fn=<AddBackward0>)\n",
      "[[3.9185967e-31 1.0000000e+00]\n",
      " [3.9104731e-31 1.0000000e+00]] [1 1]\n",
      "tensor(0.1133, grad_fn=<AddBackward0>)\n",
      "[[5.80e-43 1.00e+00]\n",
      " [5.58e-43 1.00e+00]] [1 1]\n",
      "tensor(0.1133, grad_fn=<AddBackward0>)\n",
      "[[0. 1.]\n",
      " [0. 1.]] [1 1]\n",
      "tensor(0.1133, grad_fn=<AddBackward0>)\n",
      "[[0. 1.]\n",
      " [0. 1.]] [1 1]\n",
      "tensor(0.1133, grad_fn=<AddBackward0>)\n",
      "[[0. 1.]\n",
      " [0. 1.]] [1 1]\n",
      "tensor(0.1133, grad_fn=<AddBackward0>)\n",
      "[[0. 1.]\n",
      " [0. 1.]] [1 1]\n",
      "tensor(0.1133, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    agg_loss = 0\n",
    "    for i in range(5):\n",
    "        inst_idx = random.randint(0, len(ys) - 1)\n",
    "        train_idx = torch.as_tensor(range(2), dtype=torch.int32)\n",
    "        \n",
    "        output, edge_features[inst_idx] = model(\n",
    "            node_features[inst_idx], \n",
    "            c_v_edges[inst_idx], \n",
    "            v_c_edges[inst_idx], \n",
    "            edge_features[inst_idx].detach()\n",
    "        )\n",
    "        fl = FocalLoss()\n",
    "        loss = fl(output[train_idx], ys[inst_idx][train_idx])\n",
    "        agg_loss += loss\n",
    "        \n",
    "    print(output[train_idx].detach().numpy(), ys[inst_idx][train_idx].detach().numpy())\n",
    "    print(agg_loss)\n",
    "    \n",
    "    agg_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 1.0000e+00],\n",
       "        [0.0000e+00, 1.0000e+00],\n",
       "        [3.9067e-38, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1], dtype=torch.int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 1.0000e+00],\n",
       "        [0.0000e+00, 1.0000e+00],\n",
       "        [3.9067e-38, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert 1 == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, num):\n",
    "    global data_edge_features\n",
    "    t = time.time()\n",
    "\n",
    "    output, data_edge_features[num] = model(data_features[num], data_edge_A[num], data_edge_B[num], data_edge_features[num].detach())\n",
    "    print(data_solution[num][idx_train])\n",
    "\n",
    "    lf = Focal_Loss(torch.as_tensor(data_labels[num]))\n",
    "    loss_train = lf(output[idx_train], data_solution[num][idx_train])\n",
    "\n",
    "    return loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = time.time()\n",
    "loss_values = []\n",
    "for epoch in range(args.epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    now_loss = 0\n",
    "    \n",
    "    for i in range(5):\n",
    "        now_data = random.randint(0, data_num - 1)\n",
    "        now_loss += train(epoch, now_data)\n",
    "        \n",
    "    loss_values.append(now_loss)\n",
    "    now_loss.backward()\n",
    "    optimizer.step()\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(now_loss))\n",
    "\n",
    "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
    "    if loss_values[-1] < best:\n",
    "        best = loss_values[-1]\n",
    "        best_epoch = epoch\n",
    "        bad_counter = 0\n",
    "    else:\n",
    "        bad_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "model = SpGAT(nfeat=data_features[0].shape[1],    # Feature dimension\n",
    "            nhid=args.hidden,             # Feature dimension of each hidden layer\n",
    "            nclass=int(data_solution[0].max()) + 1, # Number of classes\n",
    "            dropout=args.dropout,         # Dropout\n",
    "            nheads=args.nb_heads,         # Number of heads\n",
    "            alpha=args.alpha)             # LeakyReLU alpha coefficient\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),    \n",
    "                       lr=args.lr,                        # Learning rate\n",
    "                       weight_decay=args.weight_decay)    # Weight decay to prevent overfitting\n",
    "\n",
    "if args.cuda: # Move to GPU\n",
    "    model.to(device)\n",
    "    for now_data in range(data_num):\n",
    "        data_features[now_data] = data_features[now_data].to(device)\n",
    "        data_labels[now_data] = data_labels[now_data].to(device)\n",
    "        data_solution[now_data] = data_solution[now_data].to(device)\n",
    "        data_edge_A[now_data] = data_edge_A[now_data].to(device)\n",
    "        data_edge_B[now_data] = data_edge_B[now_data].to(device)\n",
    "        data_edge_features[now_data] = data_edge_features[now_data].to(device)\n",
    "        data_idx_train[now_data] = data_idx_train[now_data].to(device)\n",
    "\n",
    "\n",
    "for now_data in range(data_num):\n",
    "    data_features[now_data] = Variable(data_features[now_data])\n",
    "    data_edge_A[now_data] = Variable(data_edge_A[now_data])\n",
    "    data_edge_B[now_data] = Variable(data_edge_B[now_data])\n",
    "    data_solution[now_data] = Variable(data_solution[now_data])\n",
    "    # Define computation graph for automatic differentiation\n",
    "\n",
    "def train(epoch, num):\n",
    "    global data_edge_features\n",
    "    t = time.time()\n",
    "\n",
    "    output, data_edge_features[num] = model(data_features[num], data_edge_A[num], data_edge_B[num], data_edge_features[num].detach())\n",
    "    print(data_solution[num][idx_train])\n",
    "\n",
    "    lf = Focal_Loss(torch.as_tensor(data_labels[num]))\n",
    "    loss_train = lf(output[idx_train], data_solution[num][idx_train])\n",
    "\n",
    "    return loss_train\n",
    "\n",
    "t_total = time.time()\n",
    "loss_values = []\n",
    "bad_counter = 0\n",
    "best = args.epochs + 1\n",
    "best_epoch = 0\n",
    "for epoch in range(args.epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    now_loss = 0\n",
    "    for i in range(5):\n",
    "        now_data = random.randint(0, data_num - 1)\n",
    "        now_loss += train(epoch, now_data)\n",
    "    loss_values.append(now_loss)\n",
    "    now_loss.backward()\n",
    "    optimizer.step()\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(now_loss))\n",
    "\n",
    "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
    "    if loss_values[-1] < best:\n",
    "        best = loss_values[-1]\n",
    "        best_epoch = epoch\n",
    "        bad_counter = 0\n",
    "    else:\n",
    "        bad_counter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
