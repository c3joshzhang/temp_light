{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torch_geometric torch-scatter pandas scikit-learn wandb\n",
    "# python -m pip install gurobipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from temp.data.gen import parallel_generate_problem, parallel_generate_solutions\n",
    "from temp.data.problem import setcover\n",
    "from temp.data.info import ModelInfo\n",
    "from temp.data.dataset import ModelGraphDataset\n",
    "from temp.data.aug import parallel_augment_info, augment_info\n",
    "\n",
    "\n",
    "from temp.solver.utils import solve_inst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAIN_DIR = \"temp/pre_train\"\n",
    "TRAIN_DIR = \"temp/train\"\n",
    "VALID_DIR = \"temp/valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel_generate_problem(setcover, PRE_TRAIN_DIR, 100, 10)\n",
    "# parallel_generate_solutions(PRE_TRAIN_DIR, 10)\n",
    "\n",
    "# parallel_generate_problem(setcover, TRAIN_DIR, 10000, 10)\n",
    "# parallel_generate_solutions(TRAIN_DIR, 10)\n",
    "\n",
    "# parallel_generate_problem(setcover, VALID_DIR, 100, 10)\n",
    "# parallel_generate_solutions(VALID_DIR, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Set parameter PoolSolutions to value 10\n",
      "Set parameter PoolSearchMode to value 2\n",
      "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (mac64[x86] - Darwin 22.4.0 22E252)\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Non-default parameters:\n",
      "PoolSearchMode  2\n",
      "\n",
      "Optimize a model with 128 rows, 256 columns and 3261 nonzeros\n",
      "Model fingerprint: 0x6df325b4\n",
      "Variable types: 0 continuous, 256 integer (256 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [5e+00, 2e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+00]\n",
      "Found heuristic solution: objective 221.0000000\n",
      "Presolve time: 0.00s\n",
      "Presolved: 128 rows, 256 columns, 3261 nonzeros\n",
      "Variable types: 0 continuous, 256 integer (256 binary)\n",
      "\n",
      "Root relaxation: objective 6.253703e+01, 194 iterations, 0.01 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0   62.53703    0   55  221.00000   62.53703  71.7%     -    0s\n",
      "H    0     0                     124.0000000   62.53703  49.6%     -    0s\n",
      "H    0     0                     100.0000000   62.53703  37.5%     -    0s\n",
      "H    0     0                      98.0000000   62.53703  36.2%     -    0s\n",
      "H    0     0                      89.0000000   62.53703  29.7%     -    0s\n",
      "H    0     0                      84.0000000   62.53703  25.6%     -    0s\n",
      "H    0     0                      82.0000000   62.53703  23.7%     -    0s\n",
      "     0     0   64.10778    0   56   82.00000   64.10778  21.8%     -    0s\n",
      "H    0     0                      75.0000000   64.10778  14.5%     -    0s\n",
      "     0     0   65.88329    0   53   75.00000   65.88329  12.2%     -    0s\n",
      "     0     0   67.18990    0   56   75.00000   67.18990  10.4%     -    0s\n",
      "     0     0   67.44048    0   55   75.00000   67.44048  10.1%     -    0s\n",
      "     0     0   67.44048    0   53   75.00000   67.44048  10.1%     -    0s\n",
      "     0     0   67.44048    0   54   75.00000   67.44048  10.1%     -    0s\n",
      "     0     0   68.64236    0   54   75.00000   68.64236  8.48%     -    0s\n",
      "     0     2   68.64236    0   54   75.00000   68.64236  8.48%     -    0s\n",
      "\n",
      "Optimal solution found at node 1683 - now completing solution pool...\n",
      "\n",
      "    Nodes    |    Current Node    |      Pool Obj. Bounds     |     Work\n",
      "             |                    |   Worst                   |\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "  1683  1189   76.83953   17   28   82.00000   74.15798  9.56%  22.1    0s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 1\n",
      "  MIR: 24\n",
      "  Zero half: 7\n",
      "\n",
      "Explored 3503 nodes (60484 simplex iterations) in 0.64 seconds (0.64 work units)\n",
      "Thread count was 16 (of 16 available processors)\n",
      "\n",
      "Solution count 10: 75 75 77 ... 79\n",
      "No other solutions better than 79\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 7.500000000000e+01, best bound 7.500000000000e+01, gap 0.0000%\n"
     ]
    }
   ],
   "source": [
    "m = setcover()\n",
    "model = m\n",
    "\n",
    "# model.setParam(\"OutputFlag\", 0)\n",
    "model.setParam(\"PoolSolutions\", 10)\n",
    "model.setParam(\"PoolSearchMode\", 2)\n",
    "model.optimize()\n",
    "\n",
    "vs = model.getVars()\n",
    "obj_val_and_sols = []\n",
    "\n",
    "for i in range(model.SolCount):\n",
    "    # TODO: setting solution number actually takes long time try to optimize\n",
    "    # TODO: add function to round by tolerance\n",
    "    model.params.SolutionNumber = i\n",
    "    obj_val = model.PoolObjVal\n",
    "    s = [obj_val] + [v.Xn for v in vs]\n",
    "    obj_val_and_sols.append(s)\n",
    "\n",
    "info = ModelInfo.from_model(m)\n",
    "info.var_info.sols = np.array(obj_val_and_sols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from temp.data.dataset import info_to_data\n",
    "# d = info_to_data(info)\n",
    "# aug = partial(parallel_augment_info, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (1.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 0.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 0.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (1.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 0.0, 'B'), (0.0, 5.0, 'C'), (0.0, 7.0, 'C'), (0.0, 2.0, 'C'), (0.0, 5.0, 'C'), (0.0, 4.0, 'C'), (0.0, 8.0, 'C'), (0.0, 3.0, 'C'), (0.0, 6.0, 'C'), (0.0, 4.0, 'C'), (0.0, 6.0, 'C'), (0.0, 9.0, 'C'), (0.0, 3.0, 'C'), (0.0, 4.0, 'C'), (0.0, 6.0, 'C'), (0.0, 7.0, 'C'), (0.0, 3.0, 'C'), (0.0, 6.0, 'C'), (0.0, 5.0, 'C'), (0.0, 4.0, 'C'), (0.0, 4.0, 'C'), (0.0, 3.0, 'C'), (0.0, 3.0, 'C'), (0.0, 5.0, 'C'), (0.0, 5.0, 'C')]\n",
       "[([3, 7, 31, 50, 53, 56, 61, 67, 76, 80, 83, 85, 95, 107, 110, 111, 121, 125, 156, 166, 171, 175, 176, 183, 191, 195, 197, 201, 206, 216, 222, 240], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 7.0), ([19, 20, 23, 30, 88, 108, 121, 133, 136, 143, 144, 158, 165, 175, 190, 192, 212, 213, 256], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 7.0), ([30, 32, 34, 37, 55, 59, 64, 92, 109, 113, 130, 131, 157, 171, 176, 187, 192, 195, 203, 209, 213, 216, 227, 241], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([0, 3, 12, 24, 31, 46, 67, 71, 79, 87, 101, 130, 133, 158, 164, 168, 181, 184, 198, 207, 215, 222, 228, 235, 241], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([9, 32, 61, 63, 65, 68, 81, 87, 90, 93, 119, 122, 135, 153, 155, 162, 193, 208, 225, 232, 235, 238, 247], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 1.0), ([1, 4, 11, 12, 13, 63, 82, 83, 94, 102, 112, 114, 116, 118, 120, 125, 164, 167, 170, 193, 194, 215, 238, 249, 252, 253, 254, 257], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([12, 29, 34, 35, 40, 42, 46, 47, 63, 67, 79, 93, 94, 96, 121, 130, 136, 145, 151, 152, 154, 163, 179, 188, 205, 217, 229, 230, 249, 255, 258], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([5, 6, 39, 49, 52, 71, 78, 82, 90, 93, 97, 101, 107, 113, 123, 128, 131, 138, 179, 182, 194, 203, 207, 213, 217, 233, 250], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([15, 16, 35, 62, 63, 72, 78, 92, 110, 113, 126, 145, 155, 192, 194, 224, 241, 249], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([4, 18, 23, 38, 57, 58, 59, 61, 68, 72, 101, 120, 122, 128, 135, 143, 183, 186, 198, 209, 216, 221, 224, 243, 247], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([23, 49, 70, 96, 104, 120, 134, 135, 139, 149, 152, 166, 216, 217, 227, 241, 248, 250, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([0, 9, 12, 13, 21, 32, 37, 45, 57, 60, 70, 77, 93, 105, 113, 114, 135, 141, 153, 165, 170, 172, 179, 191, 205, 209, 216, 218, 221, 225, 232, 234], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 2.0), ([8, 17, 34, 38, 44, 52, 53, 65, 67, 71, 92, 101, 105, 121, 136, 157, 158, 178, 184, 189, 191, 192, 195, 205, 206, 210, 211, 215, 221], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([4, 31, 35, 44, 50, 60, 78, 80, 104, 109, 121, 122, 132, 138, 141, 154, 158, 160, 171, 176, 179, 192, 223, 230, 233, 236, 246], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([0, 5, 8, 19, 32, 37, 59, 60, 79, 85, 90, 103, 107, 119, 122, 130, 137, 149, 156, 167, 169, 170, 175, 188, 190, 197, 217, 248, 255, 259], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([29, 44, 53, 55, 82, 84, 91, 99, 106, 125, 131, 134, 143, 153, 166, 177, 220, 232, 238, 242, 245, 260], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([23, 28, 80, 84, 87, 90, 102, 116, 120, 130, 153, 161, 176, 203, 209, 224, 226, 238], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([7, 13, 19, 41, 52, 68, 70, 71, 81, 85, 91, 95, 99, 103, 109, 115, 124, 142, 150, 168, 179, 180, 186, 208, 215, 216, 217, 219, 220, 247, 248, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([2, 9, 28, 60, 112, 118, 121, 129, 130, 151, 164, 198, 209, 217, 229, 230, 245], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 2.0), ([32, 43, 71, 80, 131, 138, 203, 216, 219, 226, 227, 231, 252], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([12, 15, 22, 24, 33, 51, 67, 95, 104, 109, 119, 124, 149, 166, 170, 183, 188, 197, 199, 201, 222, 229, 232, 237, 250, 254], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([19, 22, 33, 36, 51, 56, 65, 98, 110, 113, 131, 133, 135, 165, 181, 195, 198, 199, 210, 213, 236, 243], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([1, 6, 18, 29, 68, 76, 78, 84, 90, 98, 102, 103, 120, 145, 146, 148, 159, 163, 189, 193, 196, 203, 204, 209, 218, 232, 243], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([10, 72, 87, 89, 93, 94, 105, 110, 114, 137, 139, 166, 172, 204, 212, 240, 247], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([0, 2, 14, 16, 17, 25, 37, 59, 64, 91, 98, 103, 108, 114, 131, 139, 164, 170, 182, 184, 198, 203, 230], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([2, 4, 14, 19, 25, 43, 53, 56, 75, 82, 92, 102, 105, 136, 161, 163, 178, 182, 188, 189, 209, 214, 239, 240, 254, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([5, 8, 9, 30, 37, 66, 74, 90, 93, 101, 116, 124, 130, 152, 155, 161, 184, 190, 195, 203, 208, 236, 251], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 1.0), ([23, 24, 30, 33, 43, 51, 62, 91, 102, 113, 114, 120, 126, 137, 138, 145, 152, 153, 156, 170, 182, 184, 198, 206, 229], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([15, 28, 31, 41, 53, 59, 63, 64, 67, 75, 82, 84, 92, 114, 116, 132, 136, 140, 162, 171, 179, 190, 204, 253, 261], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 2.0), ([2, 10, 27, 28, 37, 43, 60, 64, 66, 69, 71, 99, 111, 127, 134, 137, 156, 162, 179, 239], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([3, 11, 13, 42, 44, 51, 63, 75, 84, 120, 130, 133, 141, 147, 149, 162, 179, 181, 192, 194, 211, 215, 262], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([11, 13, 19, 22, 68, 75, 80, 81, 104, 108, 126, 132, 136, 137, 141, 152, 154, 157, 166, 167, 193, 198, 203, 209, 236, 238], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([5, 14, 16, 21, 23, 34, 43, 57, 86, 94, 122, 131, 134, 138, 143, 145, 156, 157, 161, 163, 169, 185, 206, 210, 221, 226, 247, 252], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 10.0), ([1, 4, 7, 13, 34, 40, 47, 61, 68, 81, 89, 92, 103, 105, 111, 122, 124, 154, 169, 179, 183, 187, 210, 213, 247, 251, 263], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([1, 3, 5, 9, 12, 16, 19, 21, 26, 27, 48, 62, 65, 70, 88, 104, 107, 133, 141, 156, 159, 163, 170, 177, 180, 181, 191, 193, 216, 217, 228, 245], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([19, 33, 53, 71, 75, 107, 118, 119, 121, 140, 141, 164, 165, 175, 186, 193, 197, 208, 213], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([45, 48, 71, 90, 98, 101, 102, 117, 156, 165, 170, 182, 193, 197, 201, 212, 221, 222, 229, 234, 235, 247], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([11, 13, 17, 21, 44, 55, 60, 61, 65, 68, 87, 117, 127, 153, 155, 164, 173, 176, 184, 192, 202, 210], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([0, 7, 8, 16, 18, 24, 34, 42, 45, 48, 55, 73, 86, 91, 93, 107, 132, 136, 151, 164, 173, 175, 194, 206, 212, 215, 222, 227, 254], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([13, 33, 55, 73, 92, 127, 132, 136, 141, 149, 159, 188, 201, 225, 233, 239, 240], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([22, 33, 48, 62, 84, 97, 99, 146, 150, 153, 169, 178, 191, 203, 204, 208, 216, 229, 237, 238, 264], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 2.0), ([7, 11, 45, 50, 61, 65, 66, 72, 81, 90, 96, 126, 135, 138, 142, 144, 146, 155, 163, 169, 176, 188, 194, 196, 202, 208, 215, 229, 252], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([25, 31, 54, 65, 69, 83, 84, 108, 111, 115, 133, 137, 144, 147, 148, 157, 172, 194, 212, 215, 217, 221, 224, 228, 232, 253], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 7.0), ([11, 23, 26, 45, 50, 53, 55, 64, 70, 87, 96, 97, 102, 133, 140, 141, 174, 175, 176, 193, 194, 197, 199, 215, 222, 236, 238], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([10, 45, 52, 53, 69, 82, 85, 140, 150, 163, 166, 178, 180, 199, 200, 205, 222, 227, 229, 238, 242, 248, 252, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([6, 45, 57, 65, 81, 82, 105, 123, 129, 135, 160, 168, 170, 172, 180, 205, 208, 212, 216, 221, 230, 239, 242], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([8, 12, 16, 30, 34, 42, 60, 72, 99, 106, 113, 119, 148, 158, 175, 189, 192, 232, 241, 265], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([18, 53, 57, 64, 73, 74, 89, 96, 99, 100, 101, 108, 111, 124, 133, 147, 158, 168, 179, 189, 195, 215, 221, 236, 245, 246, 252, 254, 266], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([12, 16, 17, 51, 59, 69, 73, 78, 85, 105, 110, 128, 133, 134, 138, 152, 172, 184, 201, 205, 215, 223, 225, 226, 227, 228, 241, 248], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 10.0), ([9, 22, 23, 50, 65, 70, 93, 98, 105, 110, 111, 132, 133, 189, 192, 197, 224, 225, 234], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([32, 35, 43, 45, 49, 55, 80, 89, 95, 99, 119, 146, 156, 173, 177, 195, 198, 218, 223, 233, 239, 246, 251], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 2.0), ([9, 15, 21, 30, 31, 42, 55, 65, 86, 88, 90, 97, 98, 100, 103, 106, 112, 116, 122, 170, 173, 191, 194, 206, 221, 229], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([0, 1, 4, 12, 20, 49, 56, 76, 77, 83, 109, 113, 121, 126, 143, 150, 157, 177, 180, 195, 231, 232, 245, 248, 267], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([6, 9, 21, 29, 47, 50, 71, 78, 85, 86, 104, 105, 108, 113, 124, 126, 145, 148, 168, 171, 176, 187, 191, 208, 218], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([2, 3, 9, 12, 21, 28, 34, 37, 39, 59, 63, 75, 80, 88, 89, 106, 115, 117, 124, 139, 166, 174, 178, 179, 188, 190, 195, 197, 224, 236, 252], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 9.0), ([18, 25, 49, 59, 88, 96, 100, 107, 113, 124, 133, 137, 143, 147, 155, 177, 188, 199, 211, 215, 221, 254], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([9, 47, 49, 55, 62, 74, 84, 85, 88, 97, 111, 123, 134, 135, 149, 151, 170, 171, 195, 196, 208, 230, 250, 254], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([1, 7, 8, 25, 26, 32, 34, 58, 89, 109, 110, 115, 126, 146, 152, 170, 185, 186, 197, 204, 243, 244, 250], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([14, 24, 25, 33, 34, 38, 66, 70, 74, 89, 94, 107, 129, 135, 155, 156, 166, 173, 195, 223, 230, 246, 268], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 2.0), ([3, 25, 36, 43, 44, 46, 48, 54, 56, 75, 83, 93, 107, 111, 122, 132, 145, 152, 190, 193, 205, 208, 216, 244], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([14, 17, 21, 26, 32, 34, 36, 38, 43, 51, 55, 59, 76, 87, 93, 103, 116, 120, 121, 133, 137, 155, 165, 166, 171, 172, 173, 182, 189, 190, 201, 210, 219, 229, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([0, 1, 4, 20, 27, 36, 40, 52, 56, 69, 101, 104, 110, 116, 118, 128, 153, 158, 202, 205, 214, 233], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 7.0), ([0, 5, 15, 25, 31, 43, 53, 66, 73, 75, 87, 110, 126, 128, 137, 139, 152, 156, 174, 175, 178, 181, 184, 187, 190, 195, 204, 217, 219, 237, 243, 246, 248, 250, 254], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([4, 48, 50, 51, 53, 67, 74, 97, 110, 121, 146, 149, 152, 161, 165, 167, 170, 183, 200, 210, 227, 233, 254], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([10, 20, 21, 30, 32, 37, 49, 64, 67, 85, 86, 93, 111, 112, 141, 144, 153, 157, 188, 229, 237, 247], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([19, 21, 33, 42, 64, 72, 86, 104, 109, 135, 157, 175, 178, 186, 215, 231, 238, 239], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 2.0), ([33, 40, 43, 45, 46, 49, 56, 84, 88, 90, 96, 100, 102, 177, 191, 196, 211, 237, 249, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 1.0), ([20, 28, 68, 108, 116, 146, 153, 158, 182, 184, 210, 215, 218, 229, 244, 247, 254, 269], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([1, 6, 29, 35, 61, 65, 77, 82, 94, 107, 122, 124, 128, 156, 160, 163, 172, 199, 207, 213, 218, 236, 238, 252, 254], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 7.0), ([38, 39, 55, 59, 67, 74, 85, 88, 96, 100, 102, 106, 113, 120, 124, 125, 126, 141, 144, 151, 165, 188, 198, 200, 204, 215, 220, 229, 242, 245], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([9, 12, 66, 95, 106, 121, 132, 147, 151, 153, 166, 173, 195, 197, 231, 248], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([5, 15, 20, 27, 58, 73, 89, 103, 109, 112, 113, 117, 118, 139, 141, 149, 176, 180, 181, 187, 192, 198, 208, 230, 233, 234, 249], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([16, 20, 39, 48, 58, 105, 122, 132, 146, 148, 153, 187, 193, 198, 270], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([5, 8, 9, 38, 46, 66, 81, 84, 127, 128, 131, 152, 179, 180, 181, 187, 192, 194, 201, 226, 231, 233, 252], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([9, 20, 45, 53, 82, 84, 92, 96, 102, 103, 104, 112, 113, 141, 154, 163, 177, 191, 205, 223, 233, 251], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([4, 16, 28, 30, 38, 51, 69, 78, 93, 114, 130, 131, 133, 137, 143, 158, 160, 172, 186, 188, 204, 206, 232, 233, 238, 246, 251, 253], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 7.0), ([4, 22, 36, 53, 66, 68, 70, 89, 98, 108, 115, 131, 134, 138, 145, 147, 153, 164, 166, 167, 174, 196, 203, 217, 220, 223], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([6, 8, 16, 19, 21, 31, 45, 80, 85, 90, 94, 119, 128, 142, 165, 166, 172, 179, 181, 182, 184, 185, 188, 202, 204, 210, 216, 229], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 10.0), ([0, 1, 14, 27, 29, 32, 36, 59, 67, 70, 81, 83, 87, 131, 139, 151, 157, 169, 180, 190, 201, 208, 241, 252], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([5, 18, 20, 45, 47, 55, 58, 79, 84, 85, 96, 117, 164, 177, 189, 192, 203, 205, 224, 232, 233], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([2, 5, 21, 23, 25, 26, 46, 49, 65, 76, 81, 87, 95, 105, 111, 134, 137, 139, 152, 153, 165, 166, 167, 175, 180, 186, 192, 196, 198, 208, 222, 230, 233, 246], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([1, 17, 26, 35, 42, 43, 49, 60, 65, 72, 82, 85, 86, 92, 110, 119, 130, 132, 147, 166, 173, 185, 189, 193, 197, 199, 215, 230, 241, 246, 249], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([4, 13, 21, 49, 52, 65, 73, 84, 97, 100, 112, 118, 121, 125, 130, 133, 134, 144, 150, 151, 152, 154, 163, 172, 176, 180, 192, 203, 207, 229, 240, 246], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 9.0), ([5, 14, 18, 39, 93, 96, 101, 105, 109, 121, 130, 137, 147, 149, 156, 164, 172, 180, 195, 211, 235, 246, 249, 271], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 1.0), ([9, 26, 27, 34, 40, 47, 51, 55, 66, 67, 73, 88, 91, 106, 108, 111, 122, 128, 158, 165, 168, 191, 193, 219, 252], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 7.0), ([0, 1, 2, 3, 4, 15, 38, 47, 63, 65, 67, 83, 100, 107, 133, 144, 161, 169, 178, 182, 186, 193, 197, 201, 206, 218, 231, 235, 236, 246], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([0, 1, 11, 29, 35, 50, 56, 58, 62, 63, 68, 81, 89, 91, 94, 102, 104, 111, 113, 121, 136, 138, 152, 159, 170, 172, 174, 176, 180, 185, 193, 214, 229, 236, 238, 246, 254, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([2, 7, 11, 19, 26, 28, 35, 37, 43, 48, 72, 84, 90, 92, 99, 102, 110, 116, 152, 158, 183, 187, 195, 197, 201, 213, 216, 231, 238], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([16, 25, 55, 57, 64, 70, 72, 95, 97, 109, 111, 114, 115, 162, 168, 182, 196, 203, 207, 220, 236, 247], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([7, 12, 13, 17, 38, 42, 75, 76, 100, 113, 145, 151, 153, 159, 174, 175, 176, 218, 226, 251, 272], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([18, 20, 22, 27, 56, 92, 99, 119, 145, 153, 157, 170, 172, 176, 201, 203, 230, 240], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([23, 27, 29, 33, 34, 55, 56, 57, 62, 80, 89, 90, 92, 95, 96, 102, 108, 119, 134, 167, 179, 191, 198, 206, 216, 221, 228, 243], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([0, 8, 30, 38, 44, 77, 83, 106, 108, 110, 121, 127, 139, 145, 162, 181, 183, 187, 193, 200, 210, 212, 214, 227, 239], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 9.0), ([6, 13, 14, 25, 39, 45, 53, 57, 63, 64, 73, 89, 94, 111, 113, 116, 117, 133, 138, 140, 146, 159, 166, 179, 184, 199, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([1, 9, 21, 31, 35, 54, 57, 83, 112, 119, 129, 136, 155, 160, 164, 165, 190, 192, 207, 208, 231, 254], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([8, 29, 83, 87, 89, 94, 97, 100, 118, 131, 137, 141, 159, 183, 192, 199, 201, 219, 225, 227, 233, 242, 251, 253, 273], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([1, 3, 8, 29, 44, 53, 60, 87, 108, 126, 132, 151, 178, 185, 196, 197, 201, 203, 241], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([0, 7, 10, 23, 34, 42, 50, 74, 81, 82, 84, 89, 90, 94, 103, 105, 126, 132, 159, 189, 200, 204, 205, 214, 219, 235, 238, 242], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([7, 61, 67, 70, 78, 81, 118, 122, 146, 169, 182, 195, 197, 212, 223, 238], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 2.0), ([5, 13, 14, 18, 23, 45, 60, 71, 73, 98, 103, 106, 111, 121, 143, 160, 176, 190, 195, 197, 208, 209, 222, 226, 227, 241, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 7.0), ([15, 26, 48, 50, 62, 64, 66, 86, 93, 111, 116, 148, 161, 162, 172, 209, 215, 220, 236, 240, 244, 245], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([7, 20, 27, 36, 39, 60, 78, 87, 109, 117, 120, 136, 140, 146, 148, 155, 163, 166, 181, 190, 197, 208, 217, 245, 247, 251], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 7.0), ([4, 17, 44, 47, 65, 74, 83, 89, 95, 96, 98, 109, 117, 133, 137, 139, 145, 159, 169, 170, 171, 181, 184, 186, 188, 206, 222, 232, 238, 239, 245, 253], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 10.0), ([31, 43, 47, 52, 53, 61, 70, 76, 77, 93, 116, 121, 133, 135, 141, 155, 158, 160, 168, 176, 179, 187, 193, 207, 212, 217, 219, 224, 226, 274], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 7.0), ([15, 24, 32, 38, 45, 46, 50, 51, 60, 76, 96, 100, 102, 113, 114, 129, 137, 142, 166, 225, 242, 248, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([15, 23, 24, 44, 57, 60, 61, 100, 103, 116, 121, 140, 154, 157, 168, 176, 190, 191, 194, 217, 230, 242, 244, 275], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([1, 5, 9, 17, 36, 48, 49, 51, 60, 63, 70, 73, 79, 90, 92, 93, 133, 135, 143, 144, 166, 173, 178, 181, 197, 204, 214, 229, 241], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([2, 30, 51, 86, 114, 115, 123, 131, 140, 177, 179, 183, 187, 189, 195, 216, 240, 242, 245, 249, 254], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([12, 19, 20, 25, 36, 39, 46, 47, 48, 57, 74, 79, 87, 89, 90, 93, 100, 114, 115, 127, 139, 141, 152, 158, 167, 168, 171, 175, 183, 189, 195, 209, 210, 219, 238, 249, 250], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([11, 25, 35, 37, 68, 73, 76, 87, 89, 106, 117, 131, 139, 145, 148, 158, 173, 182, 190, 201, 224, 233, 241, 248, 250, 276], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([0, 3, 10, 21, 23, 31, 33, 34, 42, 44, 56, 72, 96, 117, 118, 122, 130, 134, 135, 147, 170, 198, 216, 220, 229, 235, 243], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([6, 51, 135, 145, 147, 153, 163, 202, 221, 242, 245, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([4, 6, 19, 23, 43, 53, 54, 63, 67, 82, 111, 112, 115, 116, 120, 122, 124, 138, 146, 147, 149, 168, 172, 176, 201, 224, 231, 243, 245, 246], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([4, 7, 25, 27, 44, 73, 87, 89, 97, 121, 148, 177, 222, 230, 252], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([3, 4, 10, 29, 41, 60, 65, 85, 89, 96, 110, 129, 134, 138, 153, 157, 158, 162, 179, 184, 191, 196, 218, 225, 242, 253], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([1, 13, 21, 43, 45, 46, 71, 92, 108, 119, 120, 136, 143, 151, 155, 158, 165, 183, 187, 191, 202, 209, 217, 218], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 2.0), ([9, 12, 16, 20, 36, 41, 52, 62, 80, 82, 86, 88, 90, 101, 107, 109, 111, 113, 118, 125, 137, 149, 160, 163, 176, 191, 220, 228, 244, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 7.0), ([3, 12, 25, 26, 27, 28, 37, 39, 46, 56, 57, 71, 82, 83, 86, 88, 101, 103, 116, 122, 133, 150, 154, 207, 219, 224, 234, 252], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([3, 10, 33, 48, 55, 68, 72, 78, 88, 100, 126, 129, 131, 135, 144, 145, 150, 158, 161, 163, 173, 183, 204, 206, 216, 219, 240, 241, 245], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([0, 7, 13, 16, 29, 42, 43, 67, 77, 80, 86, 88, 101, 120, 124, 128, 141, 145, 149, 162, 165, 166, 170, 180, 206, 211, 213, 225, 228, 241, 242], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([21, 29, 52, 56, 62, 63, 75, 106, 133, 135, 162, 174, 179, 191, 207, 231, 239, 246, 253, 277], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([14, 49, 68, 72, 78, 81, 85, 92, 97, 98, 135, 154, 178, 184, 202], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([11, 33, 36, 43, 50, 56, 64, 66, 100, 110, 120, 138, 147, 162, 183, 189, 194, 198, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 1.0), ([0, 3, 10, 21, 23, 39, 42, 47, 60, 64, 65, 71, 72, 75, 76, 96, 120, 153, 173, 185, 190, 199, 212, 213, 231], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([8, 10, 11, 25, 31, 38, 44, 46, 54, 55, 64, 71, 77, 94, 103, 107, 111, 112, 125, 135, 142, 152, 156, 169, 177, 196, 205, 206, 231, 232, 240, 247], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([9, 12, 15, 17, 22, 57, 60, 79, 80, 85, 107, 146, 154, 169, 194, 195, 200, 205, 217, 219, 223, 228, 231, 236, 238, 247], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([0, 6, 14, 21, 22, 25, 31, 40, 53, 57, 63, 88, 92, 97, 104, 106, 112, 113, 116, 133, 135, 140, 154, 164, 173, 186, 191, 196, 200, 228, 245, 247, 278], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([17, 60, 74, 79, 96, 112, 121, 137, 156, 179, 200, 204, 219, 244, 279], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([94, 95, 178, 181, 203, 256], [1.0, 1.0, 1.0, 1.0, 1.0, -1], '>=', 0.0), ([94, 95, 178, 181, 203, 256], [1.0, 1.0, 1.0, 1.0, 1.0, -1], '<=', 0.0), ([14, 60, 62, 149, 169, 197, 240, 257], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1], '>=', 2.0), ([14, 60, 62, 149, 169, 197, 240, 257], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1], '<=', 2.0), ([50, 169, 258], [1.0, 1.0, -1], '>=', 0.0), ([50, 169, 258], [1.0, 1.0, -1], '<=', 0.0), ([21, 106, 144, 222, 227, 259], [1.0, 1.0, 1.0, 1.0, 1.0, -1], '>=', 1.0), ([21, 106, 144, 222, 227, 259], [1.0, 1.0, 1.0, 1.0, 1.0, -1], '<=', 1.0), ([80, 132, 139, 168, 260], [1.0, 1.0, 1.0, 1.0, -1], '>=', 2.0), ([80, 132, 139, 168, 260], [1.0, 1.0, 1.0, 1.0, -1], '<=', 2.0), ([9, 26, 99, 110, 115, 153, 176, 213, 261], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1], '>=', 0.0), ([9, 26, 99, 110, 115, 153, 176, 213, 261], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1], '<=', 0.0), ([52, 65, 148, 262], [1.0, 1.0, 1.0, -1], '>=', 0.0), ([52, 65, 148, 262], [1.0, 1.0, 1.0, -1], '<=', 0.0), ([33, 114, 117, 151, 225, 254, 263], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1], '>=', 2.0), ([33, 114, 117, 151, 225, 254, 263], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1], '<=', 2.0), ([27, 41, 59, 196, 264], [1.0, 1.0, 1.0, 1.0, -1], '>=', 0.0), ([27, 41, 59, 196, 264], [1.0, 1.0, 1.0, 1.0, -1], '<=', 0.0), ([41, 84, 117, 136, 164, 204, 265], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1], '>=', 2.0), ([41, 84, 117, 136, 164, 204, 265], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1], '<=', 2.0), ([87, 95, 122, 134, 151, 169, 229, 240, 255, 266], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1], '>=', 2.0), ([87, 95, 122, 134, 151, 169, 229, 240, 255, 266], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1], '<=', 2.0), ([161, 225, 250, 267], [1.0, 1.0, 1.0, -1], '>=', 0.0), ([161, 225, 250, 267], [1.0, 1.0, 1.0, -1], '<=', 0.0), ([37, 125, 154, 171, 268], [1.0, 1.0, 1.0, 1.0, -1], '>=', 1.0), ([37, 125, 154, 171, 268], [1.0, 1.0, 1.0, 1.0, -1], '<=', 1.0), ([8, 23, 60, 133, 177, 236, 269], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1], '>=', 3.0), ([8, 23, 60, 133, 177, 236, 269], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1], '<=', 3.0), ([3, 5, 109, 177, 191, 194, 231, 270], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1], '>=', 0.0), ([3, 5, 109, 177, 191, 194, 231, 270], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1], '<=', 0.0), ([28, 75, 240, 271], [1.0, 1.0, 1.0, -1], '>=', 1.0), ([28, 75, 240, 271], [1.0, 1.0, 1.0, -1], '<=', 1.0), ([62, 73, 118, 125, 129, 212, 272], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1], '>=', 3.0), ([62, 73, 118, 125, 129, 212, 272], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1], '<=', 3.0), ([47, 66, 79, 80, 209, 273], [1.0, 1.0, 1.0, 1.0, 1.0, -1], '>=', 1.0), ([47, 66, 79, 80, 209, 273], [1.0, 1.0, 1.0, 1.0, 1.0, -1], '<=', 1.0), ([39, 81, 183, 201, 274], [1.0, 1.0, 1.0, 1.0, -1], '>=', 0.0), ([39, 81, 183, 201, 274], [1.0, 1.0, 1.0, 1.0, -1], '<=', 0.0), ([31, 38, 71, 174, 275], [1.0, 1.0, 1.0, 1.0, -1], '>=', 2.0), ([31, 38, 71, 174, 275], [1.0, 1.0, 1.0, 1.0, -1], '<=', 2.0), ([3, 150, 186, 276], [1.0, 1.0, 1.0, -1], '>=', 1.0), ([3, 150, 186, 276], [1.0, 1.0, 1.0, -1], '<=', 1.0), ([66, 194, 214, 277], [1.0, 1.0, 1.0, -1], '>=', 1.0), ([66, 194, 214, 277], [1.0, 1.0, 1.0, -1], '<=', 1.0), ([19, 67, 146, 171, 198, 278], [1.0, 1.0, 1.0, 1.0, 1.0, -1], '>=', 1.0), ([19, 67, 146, 171, 198, 278], [1.0, 1.0, 1.0, 1.0, 1.0, -1], '<=', 1.0), ([23, 173, 210, 213, 217, 279], [1.0, 1.0, 1.0, 1.0, 1.0, -1], '>=', 2.0), ([23, 173, 210, 213, 217, 279], [1.0, 1.0, 1.0, 1.0, 1.0, -1], '<=', 2.0)]\n",
       "[{0: 13.0, 1: 14.0, 2: 12.0, 3: 10.0, 4: 7.0, 5: 8.0, 6: 5.0, 7: 15.0, 8: 8.0, 9: 12.0, 10: 8.0, 11: 15.0, 12: 12.0, 13: 7.0, 14: 15.0, 15: 12.0, 16: 6.0, 17: 12.0, 18: 8.0, 19: 7.0, 20: 7.0, 21: 6.0, 22: 15.0, 23: 9.0, 24: 10.0, 25: 10.0, 26: 9.0, 27: 14.0, 28: 6.0, 29: 7.0, 30: 9.0, 31: 10.0, 32: 13.0, 33: 13.0, 34: 8.0, 35: 5.0, 36: 14.0, 37: 5.0, 38: 6.0, 39: 12.0, 40: 11.0, 41: 6.0, 42: 5.0, 43: 9.0, 44: 14.0, 45: 11.0, 46: 13.0, 47: 15.0, 48: 11.0, 49: 14.0, 50: 5.0, 51: 15.0, 52: 12.0, 53: 5.0, 54: 7.0, 55: 7.0, 56: 6.0, 57: 9.0, 58: 8.0, 59: 15.0, 60: 9.0, 61: 15.0, 62: 9.0, 63: 6.0, 64: 12.0, 65: 15.0, 66: 9.0, 67: 5.0, 68: 15.0, 69: 12.0, 70: 7.0, 71: 6.0, 72: 8.0, 73: 8.0, 74: 11.0, 75: 12.0, 76: 13.0, 77: 12.0, 78: 15.0, 79: 13.0, 80: 8.0, 81: 12.0, 82: 5.0, 83: 6.0, 84: 6.0, 85: 8.0, 86: 7.0, 87: 6.0, 88: 8.0, 89: 5.0, 90: 11.0, 91: 14.0, 92: 5.0, 93: 11.0, 94: 9.0, 95: 8.0, 96: 7.0, 97: 11.0, 98: 9.0, 99: 9.0, 100: 14.0, 101: 5.0, 102: 9.0, 103: 14.0, 104: 13.0, 105: 7.0, 106: 10.0, 107: 12.0, 108: 9.0, 109: 9.0, 110: 10.0, 111: 5.0, 112: 10.0, 113: 6.0, 114: 10.0, 115: 15.0, 116: 9.0, 117: 14.0, 118: 8.0, 119: 13.0, 120: 12.0, 121: 6.0, 122: 15.0, 123: 7.0, 124: 7.0, 125: 14.0, 126: 12.0, 127: 9.0, 128: 13.0, 129: 14.0, 130: 9.0, 131: 14.0, 132: 5.0, 133: 12.0, 134: 15.0, 135: 6.0, 136: 8.0, 137: 15.0, 138: 14.0, 139: 10.0, 140: 14.0, 141: 13.0, 142: 15.0, 143: 12.0, 144: 6.0, 145: 10.0, 146: 7.0, 147: 9.0, 148: 10.0, 149: 10.0, 150: 9.0, 151: 13.0, 152: 15.0, 153: 6.0, 154: 15.0, 155: 6.0, 156: 15.0, 157: 8.0, 158: 5.0, 159: 10.0, 160: 10.0, 161: 10.0, 162: 15.0, 163: 5.0, 164: 15.0, 165: 14.0, 166: 14.0, 167: 13.0, 168: 15.0, 169: 10.0, 170: 9.0, 171: 6.0, 172: 14.0, 173: 5.0, 174: 13.0, 175: 7.0, 176: 14.0, 177: 15.0, 178: 7.0, 179: 8.0, 180: 9.0, 181: 5.0, 182: 7.0, 183: 11.0, 184: 15.0, 185: 10.0, 186: 6.0, 187: 9.0, 188: 7.0, 189: 8.0, 190: 5.0, 191: 7.0, 192: 8.0, 193: 10.0, 194: 7.0, 195: 13.0, 196: 14.0, 197: 13.0, 198: 6.0, 199: 9.0, 200: 14.0, 201: 5.0, 202: 14.0, 203: 6.0, 204: 6.0, 205: 12.0, 206: 7.0, 207: 11.0, 208: 12.0, 209: 6.0, 210: 14.0, 211: 10.0, 212: 9.0, 213: 7.0, 214: 6.0, 215: 13.0, 216: 7.0, 217: 7.0, 218: 14.0, 219: 6.0, 220: 15.0, 221: 7.0, 222: 5.0, 223: 10.0, 224: 13.0, 225: 11.0, 226: 11.0, 227: 10.0, 228: 15.0, 229: 13.0, 230: 12.0, 231: 11.0, 232: 10.0, 233: 6.0, 234: 6.0, 235: 14.0, 236: 6.0, 237: 6.0, 238: 7.0, 239: 11.0, 240: 12.0, 241: 13.0, 242: 10.0, 243: 8.0, 244: 7.0, 245: 5.0, 246: 12.0, 247: 11.0, 248: 12.0, 249: 15.0, 250: 14.0, 251: 10.0, 252: 15.0, 253: 11.0, 254: 7.0, 255: 5.0}, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augment_info(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_d = ModelGraphDataset(PRE_TRAIN_DIR, augment=augment_info)\n",
    "train_d = ModelGraphDataset(TRAIN_DIR, augment=augment_info)\n",
    "valid_d = ModelGraphDataset(VALID_DIR)\n",
    "\n",
    "data = pre_train_d[0][1]\n",
    "var_feature_size = data.var_node_features.size(-1)\n",
    "con_feature_size = data.con_node_features.size(-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "CFG = pd.read_excel(\"temp/trained_models/setcover_model_configs.xlsx\", index_col=0).loc[0].to_dict()\n",
    "CFG[\"num_epochs\"] = 16\n",
    "CFG[\"num_layers\"] = 16\n",
    "CFG[\"hidden\"] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from temp.model.utils import get_model\n",
    "from temp.model.trainer import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name, model, criterion, optimizer, scheduler = get_model(\".\", var_feature_size, con_feature_size, n_batches=1, **CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.total_steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed()\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "pretrain_loader = DataLoader(pre_train_d, batch_size=8, shuffle=True, worker_init_fn=seed_worker, generator=torch.Generator().manual_seed(0))\n",
    "train_loader = DataLoader(train_d, batch_size=8, shuffle=True, worker_init_fn=seed_worker, generator=torch.Generator().manual_seed(0))\n",
    "val_loader = DataLoader(valid_d, batch_size=8, shuffle=True, worker_init_fn=seed_worker, generator=torch.Generator().manual_seed(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.total_steps = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Training starts on the current device cpu\n",
      ">> Pretraining for prenorm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:04<00:00,  2.95it/s]\n",
      "100%|██████████| 13/13 [00:02<00:00,  5.35it/s]\n",
      "100%|██████████| 13/13 [00:02<00:00,  6.35it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00,  8.73it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00,  8.42it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00,  6.63it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00,  7.72it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00,  9.26it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 10.45it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00,  8.57it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00,  9.54it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00,  7.43it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00,  7.59it/s]\n",
      "100%|██████████| 13/13 [00:02<00:00,  6.23it/s]\n",
      "100%|██████████| 13/13 [00:02<00:00,  5.99it/s]\n",
      "100%|██████████| 13/13 [00:02<00:00,  5.92it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00,  6.97it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00,  6.64it/s]\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 1 ----------------------------------------------------------------------------------------------------\n",
      "Training... 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [24:24<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss                0.474400\n",
      "train_acc                 0.890649\n",
      "train_f1                  0.404712\n",
      "train_precision           0.627491\n",
      "train_recall              0.298674\n",
      "train_evidence_succ       4.328710\n",
      "train_evidence_fail       2.062410\n",
      "train_uncertainty_succ    0.381471\n",
      "train_uncertainty_fail    0.542478\n",
      "train_true_bias           0.124425\n",
      "train_pred_bias           0.059260\n",
      "train_soft_pred_bias      0.104123\n",
      "train_bias_error          0.077971\n",
      "train_lr                  0.000983\n",
      "dtype: float32\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:03<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss                0.270117\n",
      "val_acc                 0.939500\n",
      "val_f1                  0.314836\n",
      "val_precision           0.674757\n",
      "val_recall              0.205318\n",
      "val_evidence_succ       7.986804\n",
      "val_evidence_fail       2.589265\n",
      "val_uncertainty_succ    0.232875\n",
      "val_uncertainty_fail    0.522410\n",
      "val_true_bias           0.067692\n",
      "val_pred_bias           0.020769\n",
      "val_soft_pred_bias      0.029590\n",
      "val_bias_error          0.046923\n",
      "val_lr                  0.000983\n",
      "dtype: float32\n",
      ">> Epoch 2 ----------------------------------------------------------------------------------------------------\n",
      "Training... 1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [15:48<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss                 0.278125\n",
      "train_acc                  0.934536\n",
      "train_f1                   0.706760\n",
      "train_precision            0.819264\n",
      "train_recall               0.621424\n",
      "train_evidence_succ       10.372229\n",
      "train_evidence_fail        2.953651\n",
      "train_uncertainty_succ     0.216323\n",
      "train_uncertainty_fail     0.496673\n",
      "train_true_bias            0.126909\n",
      "train_pred_bias            0.096247\n",
      "train_soft_pred_bias       0.100487\n",
      "train_bias_error           0.030679\n",
      "train_lr                   0.000983\n",
      "dtype: float32\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 13.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss                 0.232752\n",
      "val_acc                  0.943100\n",
      "val_f1                   0.418795\n",
      "val_precision            0.678808\n",
      "val_recall               0.302806\n",
      "val_evidence_succ       14.452123\n",
      "val_evidence_fail        3.847663\n",
      "val_uncertainty_succ     0.157342\n",
      "val_uncertainty_fail     0.472407\n",
      "val_true_bias            0.067837\n",
      "val_pred_bias            0.030385\n",
      "val_soft_pred_bias       0.034546\n",
      "val_bias_error           0.037452\n",
      "val_lr                   0.000983\n",
      "dtype: float32\n",
      ">> Epoch 3 ----------------------------------------------------------------------------------------------------\n",
      "Training... 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [13:46<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss                 0.202505\n",
      "train_acc                  0.953793\n",
      "train_f1                   0.797950\n",
      "train_precision            0.886813\n",
      "train_recall               0.725274\n",
      "train_evidence_succ       17.036173\n",
      "train_evidence_fail        3.683283\n",
      "train_uncertainty_succ     0.153245\n",
      "train_uncertainty_fail     0.470489\n",
      "train_true_bias            0.125743\n",
      "train_pred_bias            0.102825\n",
      "train_soft_pred_bias       0.106026\n",
      "train_bias_error           0.023059\n",
      "train_lr                   0.000854\n",
      "dtype: float32\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss                 0.218105\n",
      "val_acc                  0.943550\n",
      "val_f1                   0.428933\n",
      "val_precision            0.680578\n",
      "val_recall               0.313146\n",
      "val_evidence_succ       21.468081\n",
      "val_evidence_fail        4.018333\n",
      "val_uncertainty_succ     0.134413\n",
      "val_uncertainty_fail     0.464488\n",
      "val_true_bias            0.067644\n",
      "val_pred_bias            0.031298\n",
      "val_soft_pred_bias       0.036179\n",
      "val_bias_error           0.036346\n",
      "val_lr                   0.000854\n",
      "dtype: float32\n",
      ">> Epoch 4 ----------------------------------------------------------------------------------------------------\n",
      "Training... 3750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [13:25<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss                 0.181201\n",
      "train_acc                  0.957166\n",
      "train_f1                   0.814380\n",
      "train_precision            0.894060\n",
      "train_recall               0.747740\n",
      "train_evidence_succ       24.207640\n",
      "train_evidence_fail        4.125728\n",
      "train_uncertainty_succ     0.126483\n",
      "train_uncertainty_fail     0.473142\n",
      "train_true_bias            0.125630\n",
      "train_pred_bias            0.105073\n",
      "train_soft_pred_bias       0.107966\n",
      "train_bias_error           0.020611\n",
      "train_lr                   0.000629\n",
      "dtype: float32\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss                 0.202166\n",
      "val_acc                  0.945450\n",
      "val_f1                   0.488993\n",
      "val_precision            0.668374\n",
      "val_recall               0.385524\n",
      "val_evidence_succ       29.179596\n",
      "val_evidence_fail        4.067136\n",
      "val_uncertainty_succ     0.121897\n",
      "val_uncertainty_fail     0.506333\n",
      "val_true_bias            0.067692\n",
      "val_pred_bias            0.039135\n",
      "val_soft_pred_bias       0.043536\n",
      "val_bias_error           0.028558\n",
      "val_lr                   0.000629\n",
      "dtype: float32\n",
      ">> Epoch 5 ----------------------------------------------------------------------------------------------------\n",
      "Training... 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [12:27<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss                 0.160031\n",
      "train_acc                  0.961321\n",
      "train_f1                   0.836034\n",
      "train_precision            0.903110\n",
      "train_recall               0.778232\n",
      "train_evidence_succ       32.664963\n",
      "train_evidence_fail        4.354912\n",
      "train_uncertainty_succ     0.108062\n",
      "train_uncertainty_fail     0.484959\n",
      "train_true_bias            0.126694\n",
      "train_pred_bias            0.109175\n",
      "train_soft_pred_bias       0.111250\n",
      "train_bias_error           0.017522\n",
      "train_lr                   0.000371\n",
      "dtype: float32\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00,  8.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss                 0.193994\n",
      "val_acc                  0.947600\n",
      "val_f1                   0.534636\n",
      "val_precision            0.670379\n",
      "val_recall               0.444609\n",
      "val_evidence_succ       39.359928\n",
      "val_evidence_fail        4.850962\n",
      "val_uncertainty_succ     0.107443\n",
      "val_uncertainty_fail     0.514515\n",
      "val_true_bias            0.067644\n",
      "val_pred_bias            0.045192\n",
      "val_soft_pred_bias       0.044903\n",
      "val_bias_error           0.022452\n",
      "val_lr                   0.000371\n",
      "dtype: float32\n",
      ">> Epoch 6 ----------------------------------------------------------------------------------------------------\n",
      "Training... 6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [11:41<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss                 0.150282\n",
      "train_acc                  0.962791\n",
      "train_f1                   0.841738\n",
      "train_precision            0.904682\n",
      "train_recall               0.786982\n",
      "train_evidence_succ       42.648960\n",
      "train_evidence_fail        4.628588\n",
      "train_uncertainty_succ     0.097175\n",
      "train_uncertainty_fail     0.499128\n",
      "train_true_bias            0.125697\n",
      "train_pred_bias            0.109334\n",
      "train_soft_pred_bias       0.111199\n",
      "train_bias_error           0.016368\n",
      "train_lr                   0.000146\n",
      "dtype: float32\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 10.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss                 0.193744\n",
      "val_acc                  0.945750\n",
      "val_f1                   0.423792\n",
      "val_precision            0.754253\n",
      "val_recall               0.294682\n",
      "val_evidence_succ       50.183907\n",
      "val_evidence_fail        4.856821\n",
      "val_uncertainty_succ     0.097717\n",
      "val_uncertainty_fail     0.508893\n",
      "val_true_bias            0.067596\n",
      "val_pred_bias            0.026250\n",
      "val_soft_pred_bias       0.028917\n",
      "val_bias_error           0.041346\n",
      "val_lr                   0.000146\n",
      "dtype: float32\n",
      ">> Epoch 7 ----------------------------------------------------------------------------------------------------\n",
      "Training... 7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [10:40<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss                 0.141348\n",
      "train_acc                  0.964602\n",
      "train_f1                   0.848700\n",
      "train_precision            0.910555\n",
      "train_recall               0.794714\n",
      "train_evidence_succ       54.248039\n",
      "train_evidence_fail        4.812515\n",
      "train_uncertainty_succ     0.089199\n",
      "train_uncertainty_fail     0.508136\n",
      "train_true_bias            0.124886\n",
      "train_pred_bias            0.108982\n",
      "train_soft_pred_bias       0.110551\n",
      "train_bias_error           0.015904\n",
      "train_lr                   0.000017\n",
      "dtype: float32\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 10.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss                 0.189994\n",
      "val_acc                  0.943550\n",
      "val_f1                   0.353749\n",
      "val_precision            0.786260\n",
      "val_recall               0.228213\n",
      "val_evidence_succ       63.577061\n",
      "val_evidence_fail        4.732352\n",
      "val_uncertainty_succ     0.099486\n",
      "val_uncertainty_fail     0.547138\n",
      "val_true_bias            0.067644\n",
      "val_pred_bias            0.019760\n",
      "val_soft_pred_bias       0.027383\n",
      "val_bias_error           0.047885\n",
      "val_lr                   0.000017\n",
      "dtype: float32\n",
      ">> Epoch 8 ----------------------------------------------------------------------------------------------------\n",
      "Training... 8750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 28/1250 [00:19<12:22,  1.65it/s]"
     ]
    }
   ],
   "source": [
    "train(model_name, model, criterion, optimizer, scheduler, pretrain_loader, train_loader, val_loader, CFG, False, \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from temp.data.dataset import info_to_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = setcover(512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = ModelInfo.from_model(m)\n",
    "data = info_to_data(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_mask = torch.ones(len(logits), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from temp.solver import upr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, preds = upr.get_predictions(logits, binary_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "unc = upr.get_uncertainty(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = upr.get_threshold(unc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "import gurobipy as gp\n",
    "\n",
    "\n",
    "def fix_var(inst, idxs, vals):\n",
    "    assert len(idxs) == len(vals)\n",
    "    bounds = {}\n",
    "    vs = inst.getVars()\n",
    "    for idx, val in zip(idxs, vals):\n",
    "        v = vs[idx]\n",
    "        bounds[idx] = (v.lb, v.ub)\n",
    "        v.setAttr(\"lb\", val)\n",
    "        v.setAttr(\"ub\", val)\n",
    "    inst.update()\n",
    "    return bounds\n",
    "\n",
    "\n",
    "def unfix_var(inst, idxs, bounds):\n",
    "    assert len(idxs) == len(bounds)\n",
    "    vs = inst.getVars()\n",
    "    print(idxs, bounds)\n",
    "    for i, (lb, ub) in zip(idxs, bounds):\n",
    "        vs[i].setAttr(\"lb\", lb)\n",
    "        vs[i].setAttr(\"ub\", ub)\n",
    "\n",
    "\n",
    "def get_iis_vars(inst):\n",
    "    try:\n",
    "        inst.computeIIS()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        if \"Cannot compute IIS on a feasible model\" in str(e):\n",
    "            return set()\n",
    "        raise e\n",
    "\n",
    "    with NamedTemporaryFile(suffix=\".ilp\", mode=\"w+\") as f:\n",
    "        inst.write(f.name)\n",
    "        f.seek(0)\n",
    "        return set(f.read().split())\n",
    "\n",
    "\n",
    "def set_starts(inst, starts):\n",
    "    vs = inst.getVars()\n",
    "    for i, s in starts.items():\n",
    "        vs[i].setAttr(\"lb\", s)\n",
    "\n",
    "\n",
    "def solve_inst(inst):\n",
    "    vs = inst.getVars()\n",
    "    inst.optimize()\n",
    "    return inst.getAttr(\"X\", vs)\n",
    "\n",
    "\n",
    "def repair(inst, fixed: set, bounds: dict):\n",
    "    old_iis_method = getattr(inst, \"IISMethod\", -1)\n",
    "    inst.setParam(\"IISMethod\", 0)\n",
    "\n",
    "    vs = inst.getVars()\n",
    "    ns = inst.getAttr(\"varName\", vs)\n",
    "    name_to_idx = {n: i for i, n in enumerate(ns)}\n",
    "\n",
    "    freed = set()\n",
    "    while iis_var_names := get_iis_vars(inst):\n",
    "        for n in iis_var_names:\n",
    "            if n not in name_to_idx:\n",
    "                continue\n",
    "\n",
    "            var_idx = name_to_idx[n]\n",
    "            if var_idx not in fixed:\n",
    "                continue\n",
    "\n",
    "            if var_idx in freed:\n",
    "                continue\n",
    "\n",
    "            lb, ub = bounds[var_idx]\n",
    "            vs[var_idx].lb = lb\n",
    "            vs[var_idx].ub = ub\n",
    "            freed.add(var_idx)\n",
    "\n",
    "    inst.setParam(\"IISMethod\", old_iis_method)\n",
    "    return freed\n",
    "\n",
    "\n",
    "def with_lic(m, path=\"gb.lic\"):\n",
    "    with open(path) as f:\n",
    "        env = gp.Env(params=json.load(f))\n",
    "    return m.copy(env=env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from temp.solver.utils import fix_var, repair, set_starts, unfix_var, solve_inst\n",
    "\n",
    "EVIDENCE_FUNCS = {\n",
    "    \"softplus\": (lambda y: F.softplus(y)),\n",
    "    \"relu\": (lambda y: F.relu(y)),\n",
    "    \"exp\": (lambda y: torch.exp(torch.clamp(y, -10, 10))),\n",
    "}\n",
    "\n",
    "\n",
    "def get_predictions(logits, binary_mask):\n",
    "    probs = torch.softmax(logits, axis=1)\n",
    "    preds = probs[:, 1]\n",
    "    probs = _to_numpy(probs)\n",
    "    preds = _to_numpy(preds).squeeze()\n",
    "    preds[binary_mask] = preds[binary_mask].round()\n",
    "    return probs, preds\n",
    "\n",
    "\n",
    "def get_uncertainty(logits, evidence_func_name: str = \"softplus\"):\n",
    "    evidence = EVIDENCE_FUNCS[evidence_func_name](logits)\n",
    "    alpha = evidence + 1\n",
    "    uncertainty = logits.shape[1] / torch.sum(alpha, dim=1, keepdim=True)\n",
    "    return uncertainty\n",
    "\n",
    "\n",
    "def get_threshold(uncertainty: torch.Tensor, r_min: float = 0.4, r_max: float = 0.55):\n",
    "    q = (r_min + r_max) / 2\n",
    "    threshold = torch.quantile(uncertainty, q)\n",
    "    r = (uncertainty <= threshold).float().mean()\n",
    "\n",
    "    if r > r_max:\n",
    "        threshold = torch.quantile(uncertainty, r_max)\n",
    "        return threshold\n",
    "\n",
    "    if r < r_min:\n",
    "        threshold = torch.quantile(uncertainty, r_min)\n",
    "        return threshold\n",
    "\n",
    "    return threshold\n",
    "\n",
    "\n",
    "def get_confident_idx(indices, uncertainty, threshold):\n",
    "    confident_mask = uncertainty <= threshold\n",
    "    confident_idx = indices[confident_mask]\n",
    "    return confident_idx.sort()[0]\n",
    "\n",
    "\n",
    "def solve(inst, prediction, uncertainty, indices, max_iter):\n",
    "    threshold = get_threshold(uncertainty)\n",
    "    conf_idxs = get_confident_idx(indices, uncertainty, threshold)\n",
    "    conf_vals = prediction[conf_idxs]\n",
    "    bounds = fix_var(inst, conf_idxs, conf_vals)\n",
    "    print(len(bounds), len(prediction))\n",
    "\n",
    "    min_q = sum(uncertainty <= threshold) / len(uncertainty)\n",
    "    max_q = 1.0\n",
    "    dq = (max_q - min_q) / (max_iter - 1)\n",
    "\n",
    "    fixed = set(conf_idxs.tolist())\n",
    "    freed = set(repair(inst, fixed, bounds))\n",
    "    for i in range(1, max_iter):\n",
    "        sol = solve_inst(inst)\n",
    "        q = max_q - dq * i\n",
    "        threshold = np.quantile(uncertainty, q)\n",
    "        conf_idxs = get_confident_idx(indices, uncertainty, threshold)\n",
    "        to_unfix = list(fixed - set(conf_idxs.tolist()))\n",
    "        to_unfix = [i for i in to_unfix if i not in freed]\n",
    "        unfix_var(inst, to_unfix, [bounds[i] for i in to_unfix])\n",
    "        starts = {i: sol[i] for i in to_unfix}\n",
    "        starts.update({i: sol[i] for i in freed})\n",
    "        set_starts(inst, starts)\n",
    "    return sol\n",
    "\n",
    "\n",
    "def _to_numpy(tensor_obj):\n",
    "    return tensor_obj.cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter TimeLimit to value 40\n",
      "Set parameter NoRelHeurTime to value 20\n",
      "243 512\n",
      "Set parameter IISMethod to value 0\n",
      "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (mac64[x86] - Darwin 22.4.0 22E252)\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Non-default parameters:\n",
      "TimeLimit  40\n",
      "NoRelHeurTime  20\n",
      "IISMethod  0\n",
      "\n",
      "IIS computation: initial model status unknown, solving to determine model status\n",
      "Found heuristic solution: objective 0.0000000\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.00 seconds (0.00 work units)\n",
      "Thread count was 1 (of 16 available processors)\n",
      "\n",
      "Solution count 1: 0 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 0.000000000000e+00, best bound 0.000000000000e+00, gap 0.0000%\n",
      "IIS runtime: 0.01 seconds (0.00 work units)\n",
      "Cannot compute IIS on a feasible model\n",
      "Set parameter IISMethod to value -1\n",
      "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (mac64[x86] - Darwin 22.4.0 22E252)\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Non-default parameters:\n",
      "TimeLimit  40\n",
      "NoRelHeurTime  20\n",
      "\n",
      "Optimize a model with 512 rows, 512 columns and 26296 nonzeros\n",
      "Model fingerprint: 0xe45de1b5\n",
      "Variable types: 0 continuous, 512 integer (512 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [5e+00, 2e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+00]\n",
      "Found heuristic solution: objective 267.0000000\n",
      "Presolve removed 0 rows and 243 columns\n",
      "Presolve time: 0.01s\n",
      "Presolved: 512 rows, 269 columns, 14379 nonzeros\n",
      "Variable types: 0 continuous, 269 integer (269 binary)\n",
      "Starting NoRel heuristic\n",
      "Found heuristic solution: objective 250.0000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found heuristic solution: objective 241.0000000\n",
      "Found heuristic solution: objective 221.0000000\n",
      "Found heuristic solution: objective 208.0000000\n",
      "Found heuristic solution: objective 189.0000000\n",
      "Found heuristic solution: objective 183.0000000\n",
      "Found heuristic solution: objective 179.0000000\n",
      "Found heuristic solution: objective 177.0000000\n",
      "Found heuristic solution: objective 175.0000000\n",
      "Found heuristic solution: objective 148.0000000\n",
      "Found heuristic solution: objective 143.0000000\n",
      "Found heuristic solution: objective 141.0000000\n",
      "Found heuristic solution: objective 136.0000000\n",
      "Found heuristic solution: objective 134.0000000\n",
      "Found heuristic solution: objective 129.0000000\n",
      "Found heuristic solution: objective 128.0000000\n",
      "Elapsed time for NoRel heuristic: 6s (best bound 76.8308)\n",
      "Found heuristic solution: objective 127.0000000\n",
      "Elapsed time for NoRel heuristic: 11s (best bound 76.8308)\n",
      "Elapsed time for NoRel heuristic: 17s (best bound 76.8308)\n",
      "NoRel heuristic complete\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    0.0000000e+00   5.120000e+02   0.000000e+00     21s\n",
      "     833    7.6830809e+01   0.000000e+00   0.000000e+00     21s\n",
      "\n",
      "Root relaxation: objective 7.683081e+01, 833 iterations, 0.10 seconds (0.11 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0   76.83081    0  148  127.00000   76.83081  39.5%     -   21s\n",
      "     0     0   78.13084    0  144  127.00000   78.13084  38.5%     -   21s\n",
      "     0     0   78.16423    0  145  127.00000   78.16423  38.5%     -   21s\n",
      "     0     0   78.65544    0  146  127.00000   78.65544  38.1%     -   21s\n",
      "     0     0   78.65544    0  143  127.00000   78.65544  38.1%     -   21s\n",
      "     0     0   79.19467    0  146  127.00000   79.19467  37.6%     -   22s\n",
      "     0     0   79.19467    0  139  127.00000   79.19467  37.6%     -   22s\n",
      "     0     0   79.57141    0  145  127.00000   79.57141  37.3%     -   22s\n",
      "     0     0   79.61799    0  145  127.00000   79.61799  37.3%     -   22s\n",
      "     0     2   79.61799    0  145  127.00000   79.61799  37.3%     -   23s\n",
      "   414   433   84.93804   43  117  127.00000   79.61799  37.3%  65.3   25s\n",
      "  1670  1576   80.23858   22  142  127.00000   80.23858  36.8%  67.4   30s\n",
      "  4595  3567   92.57705   67  112  127.00000   80.23858  36.8%  82.8   35s\n",
      " 11446  8975   99.34235   46  103  127.00000   82.98508  34.7%  70.8   40s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 2\n",
      "  Zero half: 5\n",
      "\n",
      "Explored 11624 nodes (824096 simplex iterations) in 40.02 seconds (38.85 work units)\n",
      "Thread count was 16 (of 16 available processors)\n",
      "\n",
      "Solution count 10: 127 128 129 ... 177\n",
      "\n",
      "Time limit reached\n",
      "Best objective 1.270000000000e+02, best bound 8.300000000000e+01, gap 34.6457%\n",
      "[] []\n",
      "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (mac64[x86] - Darwin 22.4.0 22E252)\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Non-default parameters:\n",
      "TimeLimit  40\n",
      "NoRelHeurTime  20\n",
      "\n",
      "Optimize a model with 512 rows, 512 columns and 26296 nonzeros\n",
      "Model fingerprint: 0xe45de1b5\n",
      "Variable types: 0 continuous, 512 integer (512 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [5e+00, 2e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+00]\n",
      "Presolved: 512 rows, 269 columns, 14379 nonzeros\n",
      "\n",
      "Continuing optimization...\n",
      "\n",
      " 11624  9456   87.00365   23  133  127.00000   85.77470  32.5%  70.8   40s\n",
      " 15725 13170   98.77426   35  114  127.00000   86.93046  31.6%  72.0   45s\n",
      " 22667 19114  100.26665   48  111  127.00000   88.02200  30.7%  68.2   50s\n",
      " 29809 24722  106.05176   66  145  127.00000   88.58505  30.2%  67.1   65s\n",
      " 29835 24739  125.06451  139  140  127.00000   88.58505  30.2%  67.0   70s\n",
      "H29847 23509                     126.0000000   88.58505  29.7%  67.0   74s\n",
      " 29852 23513   93.02913   43  140  126.00000   88.58505  29.7%  67.0   75s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 24\n",
      "  Zero half: 1\n",
      "\n",
      "Explored 29865 nodes (2003998 simplex iterations) in 40.03 seconds (54.31 work units)\n",
      "Thread count was 16 (of 16 available processors)\n",
      "\n",
      "Solution count 10: 126 127 128 ... 175\n",
      "\n",
      "Time limit reached\n",
      "Best objective 1.260000000000e+02, best bound 8.900000000000e+01, gap 29.3651%\n",
      "[] []\n",
      "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (mac64[x86] - Darwin 22.4.0 22E252)\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Non-default parameters:\n",
      "TimeLimit  40\n",
      "NoRelHeurTime  20\n",
      "\n",
      "Optimize a model with 512 rows, 512 columns and 26296 nonzeros\n",
      "Model fingerprint: 0xe45de1b5\n",
      "Variable types: 0 continuous, 512 integer (512 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [5e+00, 2e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+00]\n",
      "Presolved: 512 rows, 269 columns, 14379 nonzeros\n",
      "\n",
      "Continuing optimization...\n",
      "\n",
      " 29866 23525   88.58505   31  145  126.00000   88.58505  29.7%  67.1   80s\n",
      " 30321 23842   88.58505   43  127  126.00000   88.58505  29.7%  68.2   85s\n",
      " 31975 24912   93.42510   59  124  126.00000   88.58505  29.7%  69.8   90s\n",
      " 37515 28155   98.57995   69  127  126.00000   88.58505  29.7%  71.6   95s\n",
      " 43351 30939     cutoff   78       126.00000   88.58505  29.7%  71.4  100s\n",
      " 50577 35455   96.57929   39  111  126.00000   89.28784  29.1%  70.4  105s\n",
      " 57794 38544   98.65240   59  113  126.00000   90.00720  28.6%  69.9  110s\n",
      " 64241 42548  114.25445   86  104  126.00000   90.50531  28.2%  71.0  115s\n",
      " 72411 46439  104.06391   62  116  126.00000   91.06147  27.7%  70.5  120s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 24\n",
      "  Flow cover: 2\n",
      "  Zero half: 1\n",
      "\n",
      "Explored 73887 nodes (5204677 simplex iterations) in 40.06 seconds (51.38 work units)\n",
      "Thread count was 16 (of 16 available processors)\n",
      "\n",
      "Solution count 10: 126 127 128 ... 175\n",
      "\n",
      "Time limit reached\n",
      "Best objective 1.260000000000e+02, best bound 9.200000000000e+01, gap 26.9841%\n",
      "[] []\n",
      "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (mac64[x86] - Darwin 22.4.0 22E252)\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Non-default parameters:\n",
      "TimeLimit  40\n",
      "NoRelHeurTime  20\n",
      "\n",
      "Optimize a model with 512 rows, 512 columns and 26296 nonzeros\n",
      "Model fingerprint: 0xe45de1b5\n",
      "Variable types: 0 continuous, 512 integer (512 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [5e+00, 2e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+00]\n",
      "Presolved: 512 rows, 269 columns, 14379 nonzeros\n",
      "\n",
      "Continuing optimization...\n",
      "\n",
      " 73887 47354  119.23802   86   96  126.00000   91.17043  27.6%  70.4  121s\n",
      " 79032 50392  106.22216   75  112  126.00000   91.36613  27.5%  71.1  125s\n",
      " 88660 54876  105.82541   72  102  126.00000   91.90691  27.1%  70.1  130s\n",
      " 95534 58630  112.70673   96   97  126.00000   92.07518  26.9%  70.0  135s\n",
      " 99110 58668  107.93693   55  109  126.00000   92.26213  26.8%  70.1  140s\n",
      "H99742 53245                     122.0000000   92.26271  24.4%  70.2  144s\n",
      " 99890 54520  102.53196   57  121  122.00000   92.26271  24.4%  70.3  145s\n",
      " 104989 58772  106.13825   59  106  122.00000   92.40409  24.3%  70.8  150s\n",
      " 111363 63621  111.09302   80  100  122.00000   92.62950  24.1%  71.8  156s\n",
      " 116492 67465   99.04427   60  106  122.00000   92.70556  24.0%  72.3  160s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 24\n",
      "  Flow cover: 2\n",
      "  Zero half: 1\n",
      "\n",
      "Explored 117743 nodes (8508455 simplex iterations) in 40.07 seconds (44.84 work units)\n",
      "Thread count was 16 (of 16 available processors)\n",
      "\n",
      "Solution count 10: 122 126 127 ... 148\n",
      "\n",
      "Time limit reached\n",
      "Best objective 1.220000000000e+02, best bound 9.300000000000e+01, gap 23.7705%\n",
      "[] []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.arange(0, len(unc), dtype=int)\n",
    "unc = unc.squeeze()\n",
    "to_solve = m.copy()\n",
    "to_solve.setParam(\"TimeLimit\", 40)\n",
    "to_solve.setParam(\"NoRelHeurTime\", 20)\n",
    "solve(to_solve, preds, unc, indices, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter TimeLimit to value 150\n",
      "Set parameter NoRelHeurTime to value 60\n",
      "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (mac64[x86] - Darwin 22.4.0 22E252)\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Non-default parameters:\n",
      "TimeLimit  150\n",
      "NoRelHeurTime  60\n",
      "\n",
      "Optimize a model with 512 rows, 512 columns and 26296 nonzeros\n",
      "Model fingerprint: 0xb6f38ccf\n",
      "Variable types: 0 continuous, 512 integer (512 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [5e+00, 2e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+00]\n",
      "Found heuristic solution: objective 300.0000000\n",
      "Presolve time: 0.16s\n",
      "Presolved: 512 rows, 512 columns, 26296 nonzeros\n",
      "Variable types: 0 continuous, 512 integer (512 binary)\n",
      "Starting NoRel heuristic\n",
      "Found heuristic solution: objective 269.0000000\n",
      "Found heuristic solution: objective 264.0000000\n",
      "Found heuristic solution: objective 233.0000000\n",
      "Found heuristic solution: objective 200.0000000\n",
      "Found heuristic solution: objective 194.0000000\n",
      "Found heuristic solution: objective 183.0000000\n",
      "Found heuristic solution: objective 154.0000000\n",
      "Found heuristic solution: objective 153.0000000\n",
      "Found heuristic solution: objective 149.0000000\n",
      "Found heuristic solution: objective 144.0000000\n",
      "Found heuristic solution: objective 138.0000000\n",
      "Found heuristic solution: objective 137.0000000\n",
      "Found heuristic solution: objective 136.0000000\n",
      "Found heuristic solution: objective 135.0000000\n",
      "Found heuristic solution: objective 134.0000000\n",
      "Found heuristic solution: objective 133.0000000\n",
      "Found heuristic solution: objective 131.0000000\n",
      "Found heuristic solution: objective 130.0000000\n",
      "Elapsed time for NoRel heuristic: 5s (best bound 76.7838)\n",
      "Elapsed time for NoRel heuristic: 11s (best bound 76.7838)\n",
      "Elapsed time for NoRel heuristic: 17s (best bound 76.7838)\n",
      "Elapsed time for NoRel heuristic: 24s (best bound 76.7838)\n",
      "Elapsed time for NoRel heuristic: 35s (best bound 76.7838)\n",
      "Found heuristic solution: objective 128.0000000\n",
      "Elapsed time for NoRel heuristic: 42s (best bound 76.7838)\n",
      "Elapsed time for NoRel heuristic: 55s (best bound 76.7838)\n",
      "Elapsed time for NoRel heuristic: 65s (best bound 76.7838)\n",
      "NoRel heuristic complete\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    0.0000000e+00   5.120000e+02   0.000000e+00     65s\n",
      "     797    7.6783843e+01   0.000000e+00   0.000000e+00     65s\n",
      "\n",
      "Root relaxation: objective 7.678384e+01, 797 iterations, 0.10 seconds (0.11 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0   76.78384    0  153  128.00000   76.78384  40.0%     -   64s\n",
      "     0     0   76.90931    0  150  128.00000   76.90931  39.9%     -   65s\n",
      "     0     0   77.06063    0  146  128.00000   77.06063  39.8%     -   65s\n",
      "     0     0   78.34860    0  144  128.00000   78.34860  38.8%     -   65s\n",
      "     0     0   78.34860    0  145  128.00000   78.34860  38.8%     -   65s\n",
      "     0     0   78.74184    0  142  128.00000   78.74184  38.5%     -   66s\n",
      "     0     0   78.78382    0  144  128.00000   78.78382  38.5%     -   66s\n",
      "     0     0   79.27743    0  143  128.00000   79.27743  38.1%     -   67s\n",
      "     0     0   79.27743    0  143  128.00000   79.27743  38.1%     -   67s\n",
      "     0     2   79.27743    0  143  128.00000   79.27743  38.1%     -   73s\n",
      "   277   286   81.35421   27  131  128.00000   79.27743  38.1%  75.4   75s\n",
      "  1167  1173   88.05670   78  125  128.00000   79.27743  38.1%  62.7   80s\n",
      "  2699  2641  122.85847  208  143  128.00000   79.27743  38.1%  73.5   87s\n",
      "  2709  2648  126.31447  232  143  128.00000   79.27743  38.1%  73.2   92s\n",
      "  2906  2789   80.92442   23  143  128.00000   79.27743  38.1%  81.6   95s\n",
      "  4149  3738   97.22398   69  136  128.00000   79.27743  38.1%   117  100s\n",
      "  6304  5069  112.16330  141  107  128.00000   79.27743  38.1%   121  105s\n",
      " 11423  8420  116.15780  177   94  128.00000   82.37863  35.6%  97.5  110s\n",
      " 15822 12312   96.57319   35  137  128.00000   85.52183  33.2%  92.9  115s\n",
      " 21202 17116  124.42704   72   84  128.00000   86.34541  32.5%  85.0  120s\n",
      " 24764 20364   92.38211   17  128  128.00000   86.91333  32.1%  92.2  125s\n",
      " 28161 23577  124.03242  138  104  128.00000   87.29556  31.8%  92.3  130s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 3\n",
      "  Zero half: 3\n",
      "\n",
      "Explored 29919 nodes (2753110 simplex iterations) in 150.05 seconds (175.47 work units)\n",
      "Thread count was 16 (of 16 available processors)\n",
      "\n",
      "Solution count 10: 128 130 131 ... 144\n",
      "\n",
      "Time limit reached\n",
      "Best objective 1.280000000000e+02, best bound 8.800000000000e+01, gap 31.2500%\n"
     ]
    }
   ],
   "source": [
    "gb_m = m.copy()\n",
    "gb_m.setParam(\"TimeLimit\", 150)\n",
    "gb_m.setParam(\"NoRelHeurTime\", 60)\n",
    "gb_m.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn4co",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
