{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torch_geometric torch-scatter pandas scikit-learn wandb\n",
    "# python -m pip install gurobipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from temp.data.gen import parallel_generate_problem, parallel_generate_solutions\n",
    "from temp.data.problem import setcover\n",
    "from temp.data.info import ModelInfo\n",
    "from temp.data.dataset import ModelGraphDataset\n",
    "from temp.data.aug import parallel_augment_info, augment_info\n",
    "\n",
    "\n",
    "from temp.solver.utils import solve_inst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAIN_DIR = \"temp/pre_train\"\n",
    "TRAIN_DIR = \"temp/train\"\n",
    "VALID_DIR = \"temp/valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel_generate_problem(setcover, PRE_TRAIN_DIR, 100, 10)\n",
    "# parallel_generate_solutions(PRE_TRAIN_DIR, 10)\n",
    "\n",
    "# parallel_generate_problem(setcover, TRAIN_DIR, 10000, 10)\n",
    "# parallel_generate_solutions(TRAIN_DIR, 10)\n",
    "\n",
    "# parallel_generate_problem(setcover, VALID_DIR, 100, 10)\n",
    "# parallel_generate_solutions(VALID_DIR, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Set parameter PoolSolutions to value 10\n",
      "Set parameter PoolSearchMode to value 2\n",
      "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (mac64[x86] - Darwin 22.4.0 22E252)\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Non-default parameters:\n",
      "PoolSearchMode  2\n",
      "\n",
      "Optimize a model with 128 rows, 256 columns and 3196 nonzeros\n",
      "Model fingerprint: 0x9c3527fa\n",
      "Variable types: 0 continuous, 256 integer (256 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [5e+00, 2e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+00]\n",
      "Found heuristic solution: objective 214.0000000\n",
      "Presolve time: 0.00s\n",
      "Presolved: 128 rows, 256 columns, 3196 nonzeros\n",
      "Variable types: 0 continuous, 256 integer (256 binary)\n",
      "Found heuristic solution: objective 203.0000000\n",
      "\n",
      "Root relaxation: objective 7.145291e+01, 206 iterations, 0.00 seconds (0.01 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0   71.45291    0   58  203.00000   71.45291  64.8%     -    0s\n",
      "H    0     0                     176.0000000   71.45291  59.4%     -    0s\n",
      "H    0     0                     120.0000000   71.45291  40.5%     -    0s\n",
      "H    0     0                     112.0000000   71.45291  36.2%     -    0s\n",
      "H    0     0                      99.0000000   71.45291  27.8%     -    0s\n",
      "H    0     0                      98.0000000   71.45291  27.1%     -    0s\n",
      "     0     0   73.64233    0   62   98.00000   73.64233  24.9%     -    0s\n",
      "H    0     0                      94.0000000   73.64233  21.7%     -    0s\n",
      "H    0     0                      92.0000000   73.64233  20.0%     -    0s\n",
      "     0     0   73.66158    0   64   92.00000   73.66158  19.9%     -    0s\n",
      "     0     0   73.66158    0   61   92.00000   73.66158  19.9%     -    0s\n",
      "     0     0   73.77306    0   64   92.00000   73.77306  19.8%     -    0s\n",
      "     0     0   75.16387    0   60   92.00000   75.16387  18.3%     -    0s\n",
      "     0     0   75.16387    0   60   92.00000   75.16387  18.3%     -    0s\n",
      "     0     0   75.16387    0   63   92.00000   75.16387  18.3%     -    0s\n",
      "     0     0   75.21181    0   64   92.00000   75.21181  18.2%     -    0s\n",
      "     0     0   75.33618    0   64   92.00000   75.33618  18.1%     -    0s\n",
      "     0     0   75.33618    0   64   92.00000   75.33618  18.1%     -    0s\n",
      "     0     0   75.46333    0   65   92.00000   75.46333  18.0%     -    0s\n",
      "     0     0   76.31313    0   67   92.00000   76.31313  17.1%     -    0s\n",
      "     0     2   76.31313    0   67   92.00000   76.31313  17.1%     -    0s\n",
      "*  355   385              37      87.0000000   76.31313  12.3%  22.3    0s\n",
      "\n",
      "Optimal solution found at node 4498 - now completing solution pool...\n",
      "\n",
      "    Nodes    |    Current Node    |      Pool Obj. Bounds     |     Work\n",
      "             |                    |   Worst                   |\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "  4498  1147     cutoff   27        92.00000   86.97843  5.46%  23.9    1s\n",
      "\n",
      "Explored 6760 nodes (139353 simplex iterations) in 1.17 seconds (1.23 work units)\n",
      "Thread count was 16 (of 16 available processors)\n",
      "\n",
      "Solution count 10: 87 89 90 ... 91\n",
      "No other solutions better than 91\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 8.700000000000e+01, best bound 8.700000000000e+01, gap 0.0000%\n"
     ]
    }
   ],
   "source": [
    "m = setcover()\n",
    "model = m\n",
    "\n",
    "# model.setParam(\"OutputFlag\", 0)\n",
    "model.setParam(\"PoolSolutions\", 10)\n",
    "model.setParam(\"PoolSearchMode\", 2)\n",
    "model.optimize()\n",
    "\n",
    "vs = model.getVars()\n",
    "obj_val_and_sols = []\n",
    "\n",
    "for i in range(model.SolCount):\n",
    "    # TODO: setting solution number actually takes long time try to optimize\n",
    "    # TODO: add function to round by tolerance\n",
    "    model.params.SolutionNumber = i\n",
    "    obj_val = model.PoolObjVal\n",
    "    s = [obj_val] + [v.Xn for v in vs]\n",
    "    obj_val_and_sols.append(s)\n",
    "\n",
    "info = ModelInfo.from_model(m)\n",
    "info.var_info.sols = np.array(obj_val_and_sols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from temp.data.dataset import info_to_data\n",
    "# d = info_to_data(info)\n",
    "# aug = partial(parallel_augment_info, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 0.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B'), (1.0, 1.0, 'B'), (0.0, 1.0, 'B'), (0.0, 1.0, 'B')]\n",
       "[([1, 17, 28, 36, 44, 56, 62, 75, 85, 101, 117, 119, 136, 143, 154, 161, 172, 173, 185, 187, 190, 201, 209, 220, 223, 229, 240], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([6, 9, 12, 16, 26, 49, 53, 63, 65, 68, 76, 98, 102, 110, 113, 115, 119, 135, 145, 149, 158, 180, 197, 222, 224, 227, 231, 232, 237], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([6, 10, 14, 39, 40, 41, 57, 64, 65, 90, 91, 104, 128, 134, 137, 166, 184, 186, 191, 211, 224, 239, 252, 254], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([19, 21, 61, 67, 68, 93, 112, 116, 118, 142, 150, 164, 169, 188, 197, 200, 208, 221, 236, 248, 251], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([19, 21, 27, 29, 36, 45, 84, 95, 121, 123, 158, 171, 175, 176, 180, 199, 200, 225, 226, 251], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([8, 11, 18, 26, 30, 45, 61, 87, 88, 103, 110, 114, 115, 146, 148, 170, 173, 201, 208, 221, 242, 248, 249, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([12, 19, 22, 23, 37, 38, 40, 64, 67, 75, 76, 87, 99, 109, 116, 117, 134, 154, 171, 175, 186, 195, 197, 202, 215, 221, 242], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([1, 7, 12, 27, 31, 44, 45, 77, 85, 87, 95, 113, 116, 119, 128, 138, 143, 148, 156, 165, 177, 186, 197, 201, 206, 208, 226, 244], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([9, 15, 28, 32, 36, 43, 86, 90, 91, 94, 101, 110, 136, 152, 166, 167, 181, 188, 212, 253, 254, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([0, 4, 8, 13, 24, 39, 42, 43, 61, 82, 83, 84, 104, 130, 148, 149, 157, 166, 173, 189, 204, 228, 235, 254], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([7, 12, 13, 35, 37, 38, 53, 54, 70, 75, 112, 126, 139, 141, 150, 214, 217, 233, 236, 243], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 1.0), ([16, 19, 40, 42, 64, 68, 114, 116, 122, 126, 129, 130, 133, 138, 139, 140, 143, 152, 153, 158, 167, 180, 187, 197, 212, 214, 217, 219, 220, 224, 230], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([4, 21, 34, 55, 59, 64, 66, 74, 93, 101, 111, 120, 130, 132, 150, 157, 178, 184, 192, 193, 194, 197, 208, 212, 213, 214, 219, 233, 235, 246, 252], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([10, 12, 19, 21, 22, 27, 45, 49, 54, 74, 87, 92, 96, 106, 107, 111, 113, 123, 134, 138, 158, 159, 180, 185, 186, 190, 202, 206, 211, 249], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([2, 20, 21, 35, 40, 53, 61, 79, 85, 86, 88, 96, 105, 138, 141, 149, 150, 165, 174, 179, 180, 181, 183, 191, 202, 206, 216, 236, 243, 245], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([7, 14, 30, 45, 50, 53, 63, 85, 94, 98, 111, 119, 123, 125, 127, 129, 133, 138, 156, 167, 168, 179, 217, 236], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([2, 23, 32, 33, 36, 61, 72, 76, 80, 85, 92, 101, 107, 119, 128, 146, 149, 166, 179, 184, 187, 203, 212, 241, 246], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([1, 13, 29, 39, 43, 54, 57, 69, 77, 82, 85, 88, 99, 114, 146, 165, 191, 194, 198, 218, 242, 244, 245, 246, 249], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([16, 17, 19, 34, 40, 53, 71, 79, 82, 92, 93, 109, 114, 119, 126, 129, 135, 152, 159, 168, 173, 195, 196, 214, 229, 231, 237], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([0, 10, 11, 15, 29, 32, 38, 51, 88, 90, 91, 97, 106, 128, 131, 136, 138, 143, 153, 168, 196, 216, 222, 229], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([7, 35, 53, 64, 68, 69, 86, 91, 92, 93, 95, 98, 121, 123, 134, 142, 150, 152, 155, 166, 180, 203, 231, 233, 234, 235, 249, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([7, 21, 26, 31, 45, 80, 81, 101, 105, 121, 132, 144, 150, 159, 172, 186, 198, 229, 232], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([12, 17, 27, 39, 40, 45, 52, 54, 59, 76, 80, 82, 88, 100, 114, 117, 140, 144, 151, 155, 160, 164, 173, 174, 179, 186, 192, 198, 202, 221, 239, 240], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 10.0), ([17, 19, 21, 31, 35, 45, 64, 83, 85, 86, 90, 122, 136, 154, 157, 178, 183, 187, 189, 193, 202, 204, 213, 228, 229, 249, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([0, 36, 40, 41, 49, 65, 80, 102, 111, 114, 115, 117, 122, 125, 134, 145, 151, 188, 202, 207, 217], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([4, 8, 16, 25, 29, 30, 31, 61, 62, 72, 76, 83, 85, 92, 116, 117, 125, 127, 155, 158, 165, 201, 229, 247, 250, 253], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 7.0), ([8, 19, 27, 32, 35, 48, 65, 91, 112, 128, 133, 136, 151, 166, 179, 180, 188, 221, 243], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([2, 3, 4, 10, 12, 30, 31, 39, 40, 48, 52, 77, 122, 140, 144, 159, 218, 229], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([33, 37, 48, 66, 69, 87, 93, 95, 105, 124, 136, 137, 139, 143, 156, 160, 172, 176, 180, 201, 219, 227, 228, 231, 239, 249, 250], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 7.0), ([5, 11, 13, 19, 44, 46, 54, 78, 82, 102, 107, 110, 112, 129, 131, 141, 150, 165, 185, 216, 224, 231, 236, 239, 242], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([46, 49, 62, 68, 81, 82, 93, 99, 101, 107, 111, 127, 136, 141, 142, 146, 151, 158, 162, 163, 222, 223, 225, 229, 230, 237, 240, 242, 243, 245, 246], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([0, 6, 8, 10, 12, 26, 33, 41, 54, 62, 85, 99, 120, 129, 174, 187, 202, 203, 214, 236], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 2.0), ([14, 18, 21, 30, 40, 62, 82, 98, 102, 129, 132, 137, 141, 153, 168, 184, 205, 207, 209, 211, 230, 245, 248, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([3, 6, 27, 35, 40, 41, 67, 69, 78, 80, 82, 85, 88, 99, 108, 114, 117, 129, 137, 140, 145, 148, 154, 169, 170, 171, 173, 176, 178, 211, 212, 216, 217, 225], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([10, 14, 28, 30, 46, 57, 58, 60, 61, 65, 78, 92, 95, 108, 120, 131, 134, 137, 139, 146, 153, 178, 181, 214, 224, 227, 239, 249], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([7, 14, 19, 25, 26, 34, 38, 48, 57, 63, 65, 69, 92, 94, 98, 105, 115, 131, 133, 138, 144, 152, 157, 176, 188, 192, 219, 231, 240, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 9.0), ([20, 22, 25, 27, 30, 55, 56, 60, 64, 80, 93, 97, 117, 121, 123, 130, 160, 171, 172, 192, 211, 225, 230, 238, 243], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 9.0), ([5, 12, 19, 24, 26, 32, 35, 41, 45, 54, 55, 61, 62, 88, 109, 113, 129, 130, 146, 167, 169, 173, 175, 188, 198, 209, 225, 228, 235, 237, 239, 251], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([1, 4, 10, 13, 22, 27, 29, 33, 34, 37, 64, 76, 91, 114, 123, 146, 150, 164, 192, 193, 210, 216, 226, 254], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([4, 9, 11, 13, 39, 58, 112, 119, 155, 184, 191, 195, 198, 201, 217, 218, 222, 230, 232, 240], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([9, 22, 39, 44, 45, 51, 56, 58, 65, 70, 81, 86, 97, 101, 102, 117, 128, 147, 163, 165, 166, 184, 204, 208, 212, 229, 238, 248], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([3, 13, 16, 23, 37, 45, 46, 67, 72, 92, 121, 138, 160, 169, 186, 197, 202, 207, 208, 212, 217, 218], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([33, 77, 94, 105, 106, 117, 138, 149, 156, 173, 185, 193, 198, 201, 236], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([15, 32, 58, 95, 99, 127, 134, 137, 138, 142, 147, 149, 158, 159, 165, 171, 174, 184, 206, 215, 219, 224, 226, 231, 249], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([1, 3, 16, 30, 31, 32, 34, 52, 57, 60, 62, 73, 79, 82, 97, 101, 113, 117, 128, 151, 160, 162, 165, 173, 184, 192, 225, 228, 232, 236, 247, 251, 252], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 9.0), ([29, 42, 59, 82, 110, 155, 161, 164, 171, 173, 179, 186, 193, 196, 200, 207, 211, 213, 230, 240], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 7.0), ([3, 16, 41, 51, 64, 65, 68, 84, 86, 106, 107, 116, 117, 122, 156, 158, 168, 176, 186, 218, 237, 254], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([0, 6, 18, 20, 26, 36, 37, 48, 61, 67, 83, 92, 94, 113, 121, 127, 160, 161, 163, 166, 168, 186, 196, 198, 224, 233, 236, 243], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 10.0), ([10, 11, 20, 51, 86, 101, 103, 106, 117, 119, 154, 167, 185, 212, 221, 226, 230, 234, 240, 246], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 9.0), ([14, 16, 35, 45, 54, 59, 60, 102, 114, 138, 139, 153, 155, 183, 184, 186, 192, 200, 206, 217, 236, 238, 245, 250], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([3, 9, 11, 14, 34, 35, 38, 39, 44, 57, 58, 60, 72, 77, 78, 80, 90, 121, 140, 145, 160, 162, 172, 182, 201, 220, 231, 241, 249, 253], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([12, 14, 19, 21, 25, 37, 41, 48, 76, 78, 80, 100, 104, 105, 127, 129, 140, 146, 147, 162, 165, 184, 216, 221], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([1, 5, 40, 50, 60, 67, 70, 82, 94, 98, 113, 120, 132, 141, 142, 158, 165, 171, 178, 201, 203, 217, 222, 231, 242], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([7, 51, 57, 80, 81, 83, 86, 98, 100, 132, 149, 150, 172, 175, 198, 203, 204, 206, 223, 228, 233, 237, 238], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([5, 30, 36, 46, 47, 51, 62, 65, 66, 71, 80, 90, 96, 102, 105, 110, 114, 120, 121, 123, 126, 128, 135, 147, 153, 154, 168, 175, 188, 196, 209, 221, 236, 252, 253, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 10.0), ([7, 9, 21, 33, 34, 38, 60, 110, 119, 142, 153, 166, 175, 185, 194, 203, 214, 236, 254], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 2.0), ([0, 29, 30, 34, 40, 61, 72, 73, 95, 148, 162, 165, 167, 197, 199, 231, 236, 252], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([13, 19, 41, 53, 61, 64, 65, 67, 69, 72, 75, 79, 113, 126, 163, 169, 172, 183, 187, 232], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([4, 8, 10, 12, 18, 24, 30, 37, 57, 58, 70, 71, 74, 86, 97, 109, 115, 127, 139, 141, 146, 160, 166, 169, 195, 204, 206, 214, 217, 221, 239, 242], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([62, 67, 69, 72, 76, 83, 92, 125, 140, 144, 149, 163, 166, 171, 175, 183, 185, 202, 215, 223, 241, 242, 243], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([0, 17, 20, 32, 44, 55, 56, 60, 61, 64, 71, 85, 96, 97, 114, 127, 164, 177, 179, 181, 192, 198, 201, 204, 210, 217, 219, 225, 231], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([15, 39, 59, 64, 69, 73, 93, 98, 101, 110, 117, 122, 131, 148, 150, 165, 174, 175, 180, 183, 203, 208, 210, 221, 225, 233, 234], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 7.0), ([22, 58, 73, 84, 90, 94, 104, 110, 121, 133, 155, 193, 211, 224, 254, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([0, 19, 42, 53, 66, 74, 81, 91, 92, 98, 99, 105, 114, 116, 123, 133, 139, 141, 152, 157, 173, 176, 209, 226, 228, 237, 239, 248], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 7.0), ([6, 11, 14, 38, 39, 48, 63, 67, 96, 111, 116, 137, 161, 192, 208, 212, 217, 225, 228, 235], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([2, 22, 32, 88, 98, 116, 131, 145, 146, 154, 199, 203, 207, 213, 214, 227, 243], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([6, 16, 19, 22, 36, 49, 63, 64, 122, 135, 140, 152, 193, 199, 203, 209, 211, 213, 231, 235, 244, 249], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([8, 13, 14, 24, 26, 30, 36, 38, 56, 60, 64, 70, 73, 83, 86, 90, 91, 92, 93, 94, 98, 104, 123, 160, 171, 177, 190, 191, 204, 225, 233, 234, 238], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([24, 25, 31, 38, 51, 65, 114, 118, 131, 135, 142, 149, 172, 180, 189, 199, 200, 202, 206, 215, 232, 238, 240, 252, 254], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([0, 5, 12, 15, 22, 29, 33, 36, 69, 76, 81, 93, 143, 159, 171, 185, 209, 210, 216, 219], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([8, 26, 33, 50, 56, 60, 72, 76, 83, 100, 106, 113, 121, 126, 136, 151, 181, 185, 197, 204, 226, 237, 244], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([6, 10, 19, 32, 38, 39, 40, 52, 63, 103, 128, 135, 137, 142, 149, 154, 171, 181, 187, 194, 200, 203, 210, 213, 216, 218, 227, 228, 246, 254], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([6, 12, 23, 27, 43, 60, 64, 66, 75, 78, 80, 102, 112, 152, 184, 197, 204, 211, 225], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([24, 38, 43, 64, 69, 76, 81, 119, 129, 133, 145, 149, 151, 155, 162, 166, 169, 172, 178, 189, 229, 243, 254], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 2.0), ([0, 9, 16, 24, 28, 30, 39, 42, 44, 51, 55, 58, 62, 71, 94, 113, 121, 138, 153, 155, 161, 178, 188, 206, 211, 253, 254], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([18, 53, 72, 74, 109, 119, 138, 144, 147, 151, 154, 156, 157, 172, 174, 178, 179, 189, 202, 219, 220, 222, 235, 239, 242, 250], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([1, 6, 16, 18, 20, 25, 29, 31, 40, 45, 51, 55, 76, 81, 116, 123, 126, 128, 136, 146, 182, 203, 206, 217, 227, 233, 236, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([0, 33, 35, 47, 58, 60, 78, 79, 91, 114, 121, 141, 157, 160, 168, 169, 170, 178, 184, 188, 200, 204, 213, 214, 215, 218, 219, 225, 233, 248], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([19, 24, 26, 27, 33, 42, 43, 61, 73, 75, 80, 83, 96, 103, 132, 136, 154, 161, 170, 188, 192, 200, 211, 215, 247], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([4, 11, 28, 36, 59, 64, 86, 108, 123, 134, 167, 171, 173, 188, 189, 191, 192, 214, 219, 224, 229, 242], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([5, 8, 9, 10, 18, 48, 51, 54, 60, 69, 72, 73, 77, 117, 141, 149, 151, 153, 161, 173, 174, 175, 192, 213, 233, 235, 254], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 7.0), ([5, 8, 32, 52, 57, 75, 78, 79, 85, 88, 97, 99, 111, 135, 156, 170, 178, 186, 223, 235], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([14, 23, 37, 68, 93, 97, 103, 135, 143, 146, 151, 155, 165, 166, 171, 176, 179, 187, 188, 195, 207, 215, 231, 248], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([2, 14, 30, 33, 38, 58, 65, 90, 112, 122, 156, 168, 176, 182, 184, 199, 200, 210, 223, 228, 240], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([1, 14, 25, 35, 55, 66, 81, 83, 84, 99, 111, 118, 133, 136, 139, 148, 151, 160, 175, 180, 186, 189, 194, 201, 205, 208, 212, 218, 228, 231, 236, 240, 241], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 7.0), ([4, 13, 33, 36, 38, 56, 71, 72, 85, 91, 107, 108, 114, 117, 126, 129, 134, 143, 145, 153, 155, 177, 208, 211, 220, 222, 227, 230, 237, 253], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([14, 20, 26, 28, 30, 41, 49, 66, 72, 101, 114, 117, 125, 140, 144, 156, 163, 184, 192, 205, 207, 232, 234, 239, 247], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 9.0), ([21, 24, 28, 33, 41, 69, 95, 105, 108, 121, 139, 144, 145, 166, 174, 176, 187, 199, 213, 217, 228, 231, 246, 248, 253, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 7.0), ([5, 13, 17, 23, 27, 40, 71, 76, 86, 92, 95, 103, 105, 109, 110, 118, 127, 130, 132, 137, 140, 141, 143, 166, 169, 186, 194, 204, 213, 216, 220, 235, 239, 251], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 11.0), ([13, 41, 47, 52, 65, 68, 69, 72, 81, 87, 88, 103, 106, 108, 117, 122, 133, 158, 171, 180, 186, 201, 237, 243], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 7.0), ([0, 7, 16, 40, 45, 56, 58, 76, 84, 90, 96, 112, 138, 143, 151, 166, 174, 175, 176, 183, 196, 210, 216, 226, 249], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([5, 6, 8, 13, 55, 67, 76, 89, 121, 124, 137, 157, 162, 169, 195, 203, 211, 212, 217, 218, 219, 224, 227, 234, 235, 238, 246], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([12, 13, 22, 27, 55, 62, 85, 86, 89, 104, 140, 141, 152, 154, 155, 178, 198, 199, 216, 222, 229, 240, 244], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 7.0), ([2, 6, 12, 26, 33, 49, 58, 74, 85, 93, 120, 142, 144, 164, 182, 186, 199, 206, 225, 237, 242, 245, 254], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([6, 7, 10, 13, 26, 43, 51, 63, 68, 90, 108, 110, 113, 114, 115, 138, 141, 146, 153, 173, 196, 210, 229, 246, 253, 254], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([1, 3, 12, 14, 29, 31, 39, 42, 58, 61, 92, 97, 109, 113, 118, 129, 131, 159, 167, 168, 200, 202, 221, 223], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([16, 23, 28, 32, 34, 50, 63, 71, 85, 88, 89, 90, 96, 126, 131, 143, 147, 158, 160, 162, 167, 189, 190, 197, 223, 224, 232, 233, 237, 247, 249], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([4, 24, 27, 37, 48, 52, 55, 64, 65, 67, 89, 91, 95, 106, 114, 151, 159, 163, 165, 175, 182, 185, 189, 198, 205, 207, 234, 241], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 9.0), ([2, 19, 21, 49, 57, 65, 70, 82, 122, 166, 167, 174, 180, 183, 193, 197, 218, 241, 244], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([3, 19, 20, 31, 41, 43, 49, 51, 67, 80, 94, 105, 114, 117, 130, 131, 132, 143, 166, 173, 185, 192, 195, 227, 230, 253], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 10.0), ([8, 9, 31, 40, 45, 52, 53, 66, 99, 105, 133, 136, 145, 170, 181, 194, 217, 234, 241, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([4, 16, 19, 32, 35, 41, 42, 43, 49, 71, 86, 115, 129, 132, 133, 135, 147, 149, 150, 170, 172, 176, 179, 182, 184, 199, 230, 234], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([1, 3, 6, 15, 17, 49, 54, 71, 76, 88, 93, 94, 100, 113, 116, 134, 139, 166, 187, 188, 189, 192, 206, 207, 211, 214, 218, 221, 235, 246, 252], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 2.0), ([12, 17, 20, 38, 45, 56, 85, 96, 107, 119, 139, 156, 160, 163, 176, 185, 187, 191, 194, 214, 217, 220, 231, 233, 253], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([9, 32, 34, 52, 56, 57, 60, 103, 105, 111, 115, 121, 126, 127, 135, 136, 149, 154, 156, 159, 168, 175, 188, 190, 209, 241, 252, 254, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([4, 12, 16, 17, 20, 59, 70, 93, 104, 112, 129, 133, 143, 149, 150, 157, 161, 192, 193, 199, 213, 217, 218, 219, 225], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([18, 23, 32, 43, 53, 83, 88, 90, 93, 95, 96, 121, 127, 132, 155, 175, 180, 182, 185, 189, 216, 223, 228, 248, 249], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([3, 17, 20, 22, 34, 36, 39, 46, 73, 99, 111, 112, 129, 146, 160, 167, 174, 194, 197, 202, 205, 221, 234, 237, 239, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([8, 9, 10, 21, 23, 24, 57, 58, 68, 79, 80, 94, 97, 121, 123, 129, 137, 144, 150, 158, 163, 173, 178, 193, 197, 203, 208, 210, 223, 224], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([6, 10, 20, 32, 50, 52, 53, 65, 76, 79, 86, 90, 100, 119, 126, 129, 137, 141, 157, 159, 165, 170, 171, 175, 181, 184, 186, 189, 191, 214, 215, 231, 249], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([0, 1, 6, 10, 16, 29, 30, 35, 38, 61, 66, 81, 84, 87, 95, 112, 119, 134, 135, 153, 154, 165, 170, 174, 203, 222, 231, 254, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([15, 42, 52, 75, 111, 112, 133, 136, 152, 164, 177, 192, 195, 209, 212, 242, 245, 247, 249, 254], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 0.0), ([58, 62, 63, 69, 79, 95, 131, 154, 160, 166, 202, 206, 217, 231, 237, 245, 248], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([2, 4, 7, 32, 34, 35, 53, 58, 66, 74, 78, 80, 84, 133, 137, 143, 173, 187, 198, 203, 208, 209, 232, 244], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([1, 14, 29, 51, 58, 62, 70, 75, 79, 84, 86, 101, 112, 113, 130, 135, 139, 148, 154, 161, 193, 211, 216, 237, 239, 248, 252], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([2, 51, 88, 91, 122, 125, 137, 166, 170, 175, 184, 204, 208, 220, 226, 230, 251, 253], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 7.0), ([4, 5, 13, 14, 24, 39, 40, 65, 68, 81, 83, 85, 92, 107, 108, 125, 126, 135, 166, 174, 179, 202], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([0, 16, 20, 30, 33, 34, 60, 68, 76, 83, 90, 95, 102, 119, 125, 131, 132, 209, 213, 215, 237, 241], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([1, 12, 16, 30, 39, 44, 45, 50, 51, 52, 64, 73, 101, 102, 115, 121, 159, 172, 175, 177, 178, 185, 188, 189, 195, 203, 213, 221], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 7.0), ([1, 4, 11, 19, 21, 25, 29, 34, 42, 54, 77, 78, 82, 105, 106, 134, 142, 153, 167, 177, 212, 218, 220, 234, 245], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([15, 43, 44, 48, 61, 78, 83, 103, 117, 124, 125, 132, 135, 168, 169, 182, 190, 205, 222, 225, 239, 245], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([2, 8, 21, 24, 27, 35, 50, 54, 57, 58, 71, 88, 93, 96, 105, 106, 120, 140, 141, 143, 148, 154, 170, 179, 185, 209, 218, 227, 249], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 5.0), ([48, 66, 95, 114, 146, 163, 182, 187, 194, 229, 249, 252, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([58, 67, 84, 90, 113, 119, 122, 124, 129, 132, 139, 150, 151, 155, 162, 168, 234, 248, 254, 255], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([14, 16, 20, 29, 51, 56, 64, 69, 77, 85, 92, 95, 97, 103, 115, 126, 127, 130, 136, 140, 152, 153, 155, 170, 199, 212, 215, 218, 224, 238], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 8.0), ([1, 17, 38, 41, 46, 79, 87, 88, 106, 137, 142, 156, 172, 198, 204, 209, 211, 215, 217, 227, 235, 245, 251], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 3.0), ([26, 30, 32, 60, 63, 74, 84, 95, 120, 122, 154, 163, 187, 193, 205, 216, 223, 227, 235, 251, 253, 254], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 6.0), ([44, 55, 78, 93, 102, 103, 105, 131, 148, 150, 157, 193, 205, 212, 225, 229, 232], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], '>=', 4.0), ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255], [14.0, 10.0, 8.0, 12.0, 14.0, 7.0, 15.0, 12.0, 14.0, 7.0, 13.0, 13.0, 12.0, 11.0, 10.0, 14.0, 5.0, 13.0, 15.0, 7.0, 15.0, 9.0, 7.0, 7.0, 14.0, 9.0, 7.0, 12.0, 12.0, 11.0, 5.0, 8.0, 5.0, 9.0, 10.0, 5.0, 13.0, 10.0, 14.0, 11.0, 5.0, 10.0, 14.0, 10.0, 13.0, 7.0, 14.0, 9.0, 13.0, 12.0, 9.0, 15.0, 13.0, 15.0, 8.0, 10.0, 15.0, 9.0, 12.0, 8.0, 12.0, 13.0, 7.0, 11.0, 14.0, 15.0, 8.0, 5.0, 8.0, 14.0, 11.0, 15.0, 11.0, 15.0, 13.0, 15.0, 10.0, 11.0, 10.0, 10.0, 15.0, 9.0, 7.0, 7.0, 7.0, 13.0, 13.0, 5.0, 8.0, 9.0, 15.0, 6.0, 8.0, 9.0, 13.0, 15.0, 15.0, 5.0, 7.0, 10.0, 13.0, 15.0, 11.0, 10.0, 11.0, 6.0, 10.0, 15.0, 10.0, 5.0, 15.0, 14.0, 13.0, 13.0, 15.0, 7.0, 8.0, 8.0, 7.0, 6.0, 14.0, 14.0, 8.0, 10.0, 11.0, 7.0, 13.0, 14.0, 7.0, 6.0, 8.0, 15.0, 13.0, 8.0, 11.0, 11.0, 7.0, 8.0, 8.0, 14.0, 11.0, 12.0, 14.0, 7.0, 10.0, 6.0, 14.0, 13.0, 7.0, 13.0, 8.0, 15.0, 9.0, 8.0, 14.0, 14.0, 10.0, 11.0, 12.0, 15.0, 13.0, 14.0, 10.0, 11.0, 8.0, 14.0, 5.0, 7.0, 7.0, 10.0, 15.0, 8.0, 6.0, 15.0, 10.0, 14.0, 15.0, 7.0, 15.0, 9.0, 15.0, 5.0, 5.0, 6.0, 8.0, 13.0, 11.0, 14.0, 5.0, 7.0, 15.0, 13.0, 13.0, 14.0, 12.0, 7.0, 5.0, 10.0, 12.0, 14.0, 6.0, 9.0, 10.0, 14.0, 11.0, 7.0, 12.0, 6.0, 7.0, 8.0, 5.0, 6.0, 12.0, 10.0, 11.0, 5.0, 13.0, 5.0, 6.0, 5.0, 6.0, 12.0, 13.0, 5.0, 5.0, 13.0, 11.0, 12.0, 9.0, 5.0, 9.0, 11.0, 14.0, 14.0, 15.0, 11.0, 15.0, 12.0, 12.0, 6.0, 5.0, 14.0, 7.0, 12.0, 8.0, 9.0, 15.0, 7.0, 15.0, 9.0, 7.0, 14.0, 9.0, 6.0, 14.0, 12.0], '<=', 533.3868762050711), ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255], [14.0, 10.0, 8.0, 12.0, 14.0, 7.0, 15.0, 12.0, 14.0, 7.0, 13.0, 13.0, 12.0, 11.0, 10.0, 14.0, 5.0, 13.0, 15.0, 7.0, 15.0, 9.0, 7.0, 7.0, 14.0, 9.0, 7.0, 12.0, 12.0, 11.0, 5.0, 8.0, 5.0, 9.0, 10.0, 5.0, 13.0, 10.0, 14.0, 11.0, 5.0, 10.0, 14.0, 10.0, 13.0, 7.0, 14.0, 9.0, 13.0, 12.0, 9.0, 15.0, 13.0, 15.0, 8.0, 10.0, 15.0, 9.0, 12.0, 8.0, 12.0, 13.0, 7.0, 11.0, 14.0, 15.0, 8.0, 5.0, 8.0, 14.0, 11.0, 15.0, 11.0, 15.0, 13.0, 15.0, 10.0, 11.0, 10.0, 10.0, 15.0, 9.0, 7.0, 7.0, 7.0, 13.0, 13.0, 5.0, 8.0, 9.0, 15.0, 6.0, 8.0, 9.0, 13.0, 15.0, 15.0, 5.0, 7.0, 10.0, 13.0, 15.0, 11.0, 10.0, 11.0, 6.0, 10.0, 15.0, 10.0, 5.0, 15.0, 14.0, 13.0, 13.0, 15.0, 7.0, 8.0, 8.0, 7.0, 6.0, 14.0, 14.0, 8.0, 10.0, 11.0, 7.0, 13.0, 14.0, 7.0, 6.0, 8.0, 15.0, 13.0, 8.0, 11.0, 11.0, 7.0, 8.0, 8.0, 14.0, 11.0, 12.0, 14.0, 7.0, 10.0, 6.0, 14.0, 13.0, 7.0, 13.0, 8.0, 15.0, 9.0, 8.0, 14.0, 14.0, 10.0, 11.0, 12.0, 15.0, 13.0, 14.0, 10.0, 11.0, 8.0, 14.0, 5.0, 7.0, 7.0, 10.0, 15.0, 8.0, 6.0, 15.0, 10.0, 14.0, 15.0, 7.0, 15.0, 9.0, 15.0, 5.0, 5.0, 6.0, 8.0, 13.0, 11.0, 14.0, 5.0, 7.0, 15.0, 13.0, 13.0, 14.0, 12.0, 7.0, 5.0, 10.0, 12.0, 14.0, 6.0, 9.0, 10.0, 14.0, 11.0, 7.0, 12.0, 6.0, 7.0, 8.0, 5.0, 6.0, 12.0, 10.0, 11.0, 5.0, 13.0, 5.0, 6.0, 5.0, 6.0, 12.0, 13.0, 5.0, 5.0, 13.0, 11.0, 12.0, 9.0, 5.0, 9.0, 11.0, 14.0, 14.0, 15.0, 11.0, 15.0, 12.0, 12.0, 6.0, 5.0, 14.0, 7.0, 12.0, 8.0, 9.0, 15.0, 7.0, 15.0, 9.0, 7.0, 14.0, 9.0, 6.0, 14.0, 12.0], '>=', 532.6806652695533)]\n",
       "[{0: 14.0, 1: 10.0, 2: 8.0, 3: 12.0, 4: 14.0, 5: 7.0, 6: 15.0, 7: 12.0, 8: 14.0, 9: 7.0, 10: 13.0, 11: 13.0, 12: 12.0, 13: 11.0, 14: 10.0, 15: 14.0, 16: 5.0, 17: 13.0, 18: 15.0, 19: 7.0, 20: 15.0, 21: 9.0, 22: 7.0, 23: 7.0, 24: 14.0, 25: 9.0, 26: 7.0, 27: 12.0, 28: 12.0, 29: 11.0, 30: 5.0, 31: 8.0, 32: 5.0, 33: 9.0, 34: 10.0, 35: 5.0, 36: 13.0, 37: 10.0, 38: 14.0, 39: 11.0, 40: 5.0, 41: 10.0, 42: 14.0, 43: 10.0, 44: 13.0, 45: 7.0, 46: 14.0, 47: 9.0, 48: 13.0, 49: 12.0, 50: 9.0, 51: 15.0, 52: 13.0, 53: 15.0, 54: 8.0, 55: 10.0, 56: 15.0, 57: 9.0, 58: 12.0, 59: 8.0, 60: 12.0, 61: 13.0, 62: 7.0, 63: 11.0, 64: 14.0, 65: 15.0, 66: 8.0, 67: 5.0, 68: 8.0, 69: 14.0, 70: 11.0, 71: 15.0, 72: 11.0, 73: 15.0, 74: 13.0, 75: 15.0, 76: 10.0, 77: 11.0, 78: 10.0, 79: 10.0, 80: 15.0, 81: 9.0, 82: 7.0, 83: 7.0, 84: 7.0, 85: 13.0, 86: 13.0, 87: 5.0, 88: 8.0, 89: 9.0, 90: 15.0, 91: 6.0, 92: 8.0, 93: 9.0, 94: 13.0, 95: 15.0, 96: 15.0, 97: 5.0, 98: 7.0, 99: 10.0, 100: 13.0, 101: 15.0, 102: 11.0, 103: 10.0, 104: 11.0, 105: 6.0, 106: 10.0, 107: 15.0, 108: 10.0, 109: 5.0, 110: 15.0, 111: 14.0, 112: 13.0, 113: 13.0, 114: 15.0, 115: 7.0, 116: 8.0, 117: 8.0, 118: 7.0, 119: 6.0, 120: 14.0, 121: 14.0, 122: 8.0, 123: 10.0, 124: 11.0, 125: 7.0, 126: 13.0, 127: 14.0, 128: 7.0, 129: 6.0, 130: 8.0, 131: 15.0, 132: 13.0, 133: 8.0, 134: 11.0, 135: 11.0, 136: 7.0, 137: 8.0, 138: 8.0, 139: 14.0, 140: 11.0, 141: 12.0, 142: 14.0, 143: 7.0, 144: 10.0, 145: 6.0, 146: 14.0, 147: 13.0, 148: 7.0, 149: 13.0, 150: 8.0, 151: 15.0, 152: 9.0, 153: 8.0, 154: 14.0, 155: 14.0, 156: 10.0, 157: 11.0, 158: 12.0, 159: 15.0, 160: 13.0, 161: 14.0, 162: 10.0, 163: 11.0, 164: 8.0, 165: 14.0, 166: 5.0, 167: 7.0, 168: 7.0, 169: 10.0, 170: 15.0, 171: 8.0, 172: 6.0, 173: 15.0, 174: 10.0, 175: 14.0, 176: 15.0, 177: 7.0, 178: 15.0, 179: 9.0, 180: 15.0, 181: 5.0, 182: 5.0, 183: 6.0, 184: 8.0, 185: 13.0, 186: 11.0, 187: 14.0, 188: 5.0, 189: 7.0, 190: 15.0, 191: 13.0, 192: 13.0, 193: 14.0, 194: 12.0, 195: 7.0, 196: 5.0, 197: 10.0, 198: 12.0, 199: 14.0, 200: 6.0, 201: 9.0, 202: 10.0, 203: 14.0, 204: 11.0, 205: 7.0, 206: 12.0, 207: 6.0, 208: 7.0, 209: 8.0, 210: 5.0, 211: 6.0, 212: 12.0, 213: 10.0, 214: 11.0, 215: 5.0, 216: 13.0, 217: 5.0, 218: 6.0, 219: 5.0, 220: 6.0, 221: 12.0, 222: 13.0, 223: 5.0, 224: 5.0, 225: 13.0, 226: 11.0, 227: 12.0, 228: 9.0, 229: 5.0, 230: 9.0, 231: 11.0, 232: 14.0, 233: 14.0, 234: 15.0, 235: 11.0, 236: 15.0, 237: 12.0, 238: 12.0, 239: 6.0, 240: 5.0, 241: 14.0, 242: 7.0, 243: 12.0, 244: 8.0, 245: 9.0, 246: 15.0, 247: 7.0, 248: 15.0, 249: 9.0, 250: 7.0, 251: 14.0, 252: 9.0, 253: 6.0, 254: 14.0, 255: 12.0}, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augment_info(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_d = ModelGraphDataset(PRE_TRAIN_DIR, augment=augment_info)\n",
    "train_d = ModelGraphDataset(TRAIN_DIR, augment=augment_info)\n",
    "valid_d = ModelGraphDataset(VALID_DIR)\n",
    "\n",
    "data = pre_train_d[0][1]\n",
    "var_feature_size = data.var_node_features.size(-1)\n",
    "con_feature_size = data.con_node_features.size(-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "CFG = pd.read_excel(\"temp/trained_models/setcover_model_configs.xlsx\", index_col=0).loc[0].to_dict()\n",
    "CFG[\"num_epochs\"] = 16\n",
    "CFG[\"num_layers\"] = 8\n",
    "CFG[\"hidden\"] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from temp.model.utils import get_model\n",
    "from temp.model.trainer import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name, model, criterion, optimizer, scheduler = get_model(\".\", var_feature_size, con_feature_size, n_batches=1, **CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.total_steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataLoader.__init__() got an unexpected keyword argument 'num_worker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     random\u001b[38;5;241m.\u001b[39mseed(worker_seed)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m----> 7\u001b[0m pretrain_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpre_train_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_init_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_d, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_worker\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, worker_init_fn\u001b[38;5;241m=\u001b[39mseed_worker, generator\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mGenerator()\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m      9\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(valid_d, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, worker_init_fn\u001b[38;5;241m=\u001b[39mseed_worker, generator\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mGenerator()\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/share/c3/v8/conda/Miniconda3-py38_4.9.2-MacOSX-x86_64/envs/gnn4co/lib/python3.10/site-packages/torch_geometric/loader/dataloader.py:87\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, follow_batch, exclude_keys, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_batch \u001b[38;5;241m=\u001b[39m follow_batch\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexclude_keys \u001b[38;5;241m=\u001b[39m exclude_keys\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCollater\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: DataLoader.__init__() got an unexpected keyword argument 'num_worker'"
     ]
    }
   ],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed()\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "pretrain_loader = DataLoader(pre_train_d, batch_size=8, num_workers=4, shuffle=True, worker_init_fn=seed_worker, generator=torch.Generator().manual_seed(0))\n",
    "train_loader = DataLoader(train_d, batch_size=8, shuffle=True, num_workers=4, worker_init_fn=seed_worker, generator=torch.Generator().manual_seed(0))\n",
    "val_loader = DataLoader(valid_d, batch_size=8, shuffle=True, worker_init_fn=seed_worker, generator=torch.Generator().manual_seed(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.total_steps = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Training starts on the current device cpu\n",
      ">> Pretraining for prenorm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 13/13 [00:03<00:00,  3.32it/s]\n",
      "100%|| 13/13 [00:01<00:00,  7.92it/s]\n",
      "100%|| 13/13 [00:01<00:00,  9.82it/s]\n",
      "100%|| 13/13 [00:01<00:00, 10.68it/s]\n",
      "100%|| 13/13 [00:01<00:00, 12.49it/s]\n",
      "100%|| 13/13 [00:01<00:00, 10.65it/s]\n",
      "100%|| 13/13 [00:01<00:00, 11.77it/s]\n",
      "100%|| 13/13 [00:00<00:00, 14.73it/s]\n",
      "100%|| 13/13 [00:00<00:00, 17.03it/s]\n",
      "100%|| 13/13 [00:00<00:00, 13.66it/s]\n",
      "100%|| 13/13 [00:00<00:00, 14.86it/s]\n",
      "100%|| 13/13 [00:00<00:00, 15.66it/s]\n",
      "100%|| 13/13 [00:01<00:00, 10.93it/s]\n",
      "100%|| 13/13 [00:01<00:00, 12.34it/s]\n",
      "100%|| 13/13 [00:00<00:00, 13.29it/s]\n",
      "100%|| 13/13 [00:00<00:00, 13.49it/s]\n",
      "100%|| 13/13 [00:01<00:00, 11.71it/s]\n",
      "100%|| 13/13 [00:00<00:00, 13.13it/s]\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 1 ----------------------------------------------------------------------------------------------------\n",
      "Training... 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 131/1250 [02:19<21:48,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> tensor(8.5702)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1250/1250 [20:20<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss                0.472834\n",
      "train_acc                 0.886497\n",
      "train_f1                  0.389831\n",
      "train_precision           0.604918\n",
      "train_recall              0.287579\n",
      "train_evidence_succ       4.521249\n",
      "train_evidence_fail       1.960626\n",
      "train_uncertainty_succ    0.372852\n",
      "train_uncertainty_fail    0.554757\n",
      "train_true_bias           0.126045\n",
      "train_pred_bias           0.059941\n",
      "train_soft_pred_bias      0.107463\n",
      "train_bias_error          0.078712\n",
      "train_lr                  0.000983\n",
      "dtype: float32\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 13/13 [00:02<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss                0.258251\n",
      "val_acc                 0.941900\n",
      "val_f1                  0.468921\n",
      "val_precision           0.615108\n",
      "val_recall              0.378877\n",
      "val_evidence_succ       8.259477\n",
      "val_evidence_fail       2.460518\n",
      "val_uncertainty_succ    0.227978\n",
      "val_uncertainty_fail    0.538578\n",
      "val_true_bias           0.067692\n",
      "val_pred_bias           0.041875\n",
      "val_soft_pred_bias      0.045501\n",
      "val_bias_error          0.025817\n",
      "val_lr                  0.000983\n",
      "dtype: float32\n",
      ">> Epoch 2 ----------------------------------------------------------------------------------------------------\n",
      "Training... 1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1250/1250 [15:15<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss                 0.266376\n",
      "train_acc                  0.938530\n",
      "train_f1                   0.729641\n",
      "train_precision            0.810268\n",
      "train_recall               0.663609\n",
      "train_evidence_succ       11.049736\n",
      "train_evidence_fail        2.810828\n",
      "train_uncertainty_succ     0.213905\n",
      "train_uncertainty_fail     0.520480\n",
      "train_true_bias            0.124956\n",
      "train_pred_bias            0.102322\n",
      "train_soft_pred_bias       0.102596\n",
      "train_bias_error           0.022652\n",
      "train_lr                   0.000983\n",
      "dtype: float32\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 13/13 [00:01<00:00,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss                 0.219292\n",
      "val_acc                  0.944550\n",
      "val_f1                   0.473659\n",
      "val_precision            0.662683\n",
      "val_recall               0.368538\n",
      "val_evidence_succ       14.968543\n",
      "val_evidence_fail        2.796568\n",
      "val_uncertainty_succ     0.167357\n",
      "val_uncertainty_fail     0.554958\n",
      "val_true_bias            0.067837\n",
      "val_pred_bias            0.038029\n",
      "val_soft_pred_bias       0.045695\n",
      "val_bias_error           0.029808\n",
      "val_lr                   0.000983\n",
      "dtype: float32\n",
      ">> Epoch 3 ----------------------------------------------------------------------------------------------------\n",
      "Training... 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1250/1250 [38:43<00:00,  1.86s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss                 0.203316\n",
      "train_acc                  0.953176\n",
      "train_f1                   0.797608\n",
      "train_precision            0.878372\n",
      "train_recall               0.730445\n",
      "train_evidence_succ       18.138504\n",
      "train_evidence_fail        3.423136\n",
      "train_uncertainty_succ     0.157095\n",
      "train_uncertainty_fail     0.515332\n",
      "train_true_bias            0.126267\n",
      "train_pred_bias            0.104993\n",
      "train_soft_pred_bias       0.107895\n",
      "train_bias_error           0.021275\n",
      "train_lr                   0.000854\n",
      "dtype: float32\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 13/13 [00:01<00:00,  8.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss                 0.226179\n",
      "val_acc                  0.937100\n",
      "val_f1                   0.350207\n",
      "val_precision            0.582474\n",
      "val_recall               0.250369\n",
      "val_evidence_succ       22.727112\n",
      "val_evidence_fail        3.546700\n",
      "val_uncertainty_succ     0.136862\n",
      "val_uncertainty_fail     0.524397\n",
      "val_true_bias            0.067644\n",
      "val_pred_bias            0.029856\n",
      "val_soft_pred_bias       0.037886\n",
      "val_bias_error           0.037788\n",
      "val_lr                   0.000854\n",
      "dtype: float32\n",
      ">> Epoch 4 ----------------------------------------------------------------------------------------------------\n",
      "Training... 3750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 78/1250 [00:54<12:57,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1250/1250 [14:26<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss                 0.175092\n",
      "train_acc                  0.958447\n",
      "train_f1                   0.820602\n",
      "train_precision            0.898035\n",
      "train_recall               0.755462\n",
      "train_evidence_succ       26.504274\n",
      "train_evidence_fail        3.953602\n",
      "train_uncertainty_succ     0.126052\n",
      "train_uncertainty_fail     0.516351\n",
      "train_true_bias            0.125780\n",
      "train_pred_bias            0.105807\n",
      "train_soft_pred_bias       0.108454\n",
      "train_bias_error           0.019979\n",
      "train_lr                   0.000629\n",
      "dtype: float32\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 13/13 [00:01<00:00, 12.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss                 0.214141\n",
      "val_acc                  0.940900\n",
      "val_f1                   0.433908\n",
      "val_precision            0.617166\n",
      "val_recall               0.334564\n",
      "val_evidence_succ       31.828829\n",
      "val_evidence_fail        4.112826\n",
      "val_uncertainty_succ     0.120697\n",
      "val_uncertainty_fail     0.517617\n",
      "val_true_bias            0.067692\n",
      "val_pred_bias            0.036635\n",
      "val_soft_pred_bias       0.040116\n",
      "val_bias_error           0.031058\n",
      "val_lr                   0.000629\n",
      "dtype: float32\n",
      ">> Epoch 5 ----------------------------------------------------------------------------------------------------\n",
      "Training... 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1250/1250 [13:34<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss                 0.161194\n",
      "train_acc                  0.960729\n",
      "train_f1                   0.829695\n",
      "train_precision            0.905527\n",
      "train_recall               0.765582\n",
      "train_evidence_succ       35.843040\n",
      "train_evidence_fail        4.348772\n",
      "train_uncertainty_succ     0.109540\n",
      "train_uncertainty_fail     0.521026\n",
      "train_true_bias            0.124919\n",
      "train_pred_bias            0.105605\n",
      "train_soft_pred_bias       0.108361\n",
      "train_bias_error           0.019314\n",
      "train_lr                   0.000371\n",
      "dtype: float32\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 13/13 [00:01<00:00,  8.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss                 0.204250\n",
      "val_acc                  0.943400\n",
      "val_f1                   0.508254\n",
      "val_precision            0.617089\n",
      "val_recall               0.432053\n",
      "val_evidence_succ       42.052574\n",
      "val_evidence_fail        4.703334\n",
      "val_uncertainty_succ     0.110928\n",
      "val_uncertainty_fail     0.534775\n",
      "val_true_bias            0.067644\n",
      "val_pred_bias            0.047644\n",
      "val_soft_pred_bias       0.048490\n",
      "val_bias_error           0.020000\n",
      "val_lr                   0.000371\n",
      "dtype: float32\n",
      ">> Epoch 6 ----------------------------------------------------------------------------------------------------\n",
      "Training... 6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 94/1250 [00:58<11:49,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|    | 644/1250 [06:54<06:29,  1.55it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCFG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/__workspace__/temp/neural_solver/temp_light/code/temp/model/trainer.py:161\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model_name, model, criterion, optimizer, scheduler, pretrain_loader, train_loader, val_loader, config, WANDB_LOG, model_dir)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>> Epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m))\n\u001b[1;32m    159\u001b[0m epoch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 161\u001b[0m train_log \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    174\u001b[0m     free_gpu_memory()\n",
      "File \u001b[0;32m~/Desktop/__workspace__/temp/neural_solver/temp_light/code/temp/model/trainer.py:77\u001b[0m, in \u001b[0;36mstep\u001b[0;34m(epoch, model, loader, optimizer, scheduler, criterion, step_type, bias_threshold, binary_pred, eval, print_log, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m loss, evidence_tuple, uncertainty \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[1;32m     73\u001b[0m     GLOBAL_STEP, graph_idx, batch, output, y, binary_pred, step_type\n\u001b[1;32m     74\u001b[0m )\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28meval\u001b[39m:\n\u001b[0;32m---> 77\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(scheduler) \u001b[38;5;129;01min\u001b[39;00m [NoamLR, lr_scheduler\u001b[38;5;241m.\u001b[39mOneCycleLR]:\n",
      "File \u001b[0;32m/usr/local/share/c3/v8/conda/Miniconda3-py38_4.9.2-MacOSX-x86_64/envs/gnn4co/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/share/c3/v8/conda/Miniconda3-py38_4.9.2-MacOSX-x86_64/envs/gnn4co/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model_name, model, criterion, optimizer, scheduler, pretrain_loader, train_loader, val_loader, CFG, False, \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from temp.data.dataset import info_to_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = setcover(512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = ModelInfo.from_model(m)\n",
    "data = info_to_data(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_mask = torch.ones(len(logits), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from temp.solver import upr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, preds = upr.get_predictions(logits, binary_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "unc = upr.get_uncertainty(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = upr.get_threshold(unc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "import gurobipy as gp\n",
    "\n",
    "\n",
    "def fix_var(inst, idxs, vals):\n",
    "    assert len(idxs) == len(vals)\n",
    "    bounds = {}\n",
    "    vs = inst.getVars()\n",
    "    for idx, val in zip(idxs, vals):\n",
    "        v = vs[idx]\n",
    "        bounds[idx] = (v.lb, v.ub)\n",
    "        v.setAttr(\"lb\", val)\n",
    "        v.setAttr(\"ub\", val)\n",
    "    inst.update()\n",
    "    return bounds\n",
    "\n",
    "\n",
    "def unfix_var(inst, idxs, bounds):\n",
    "    assert len(idxs) == len(bounds)\n",
    "    vs = inst.getVars()\n",
    "    print(idxs, bounds)\n",
    "    for i, (lb, ub) in zip(idxs, bounds):\n",
    "        vs[i].setAttr(\"lb\", lb)\n",
    "        vs[i].setAttr(\"ub\", ub)\n",
    "\n",
    "\n",
    "def get_iis_vars(inst):\n",
    "    try:\n",
    "        inst.computeIIS()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        if \"Cannot compute IIS on a feasible model\" in str(e):\n",
    "            return set()\n",
    "        raise e\n",
    "\n",
    "    with NamedTemporaryFile(suffix=\".ilp\", mode=\"w+\") as f:\n",
    "        inst.write(f.name)\n",
    "        f.seek(0)\n",
    "        return set(f.read().split())\n",
    "\n",
    "\n",
    "def set_starts(inst, starts):\n",
    "    vs = inst.getVars()\n",
    "    for i, s in starts.items():\n",
    "        vs[i].setAttr(\"lb\", s)\n",
    "\n",
    "\n",
    "def solve_inst(inst):\n",
    "    vs = inst.getVars()\n",
    "    inst.optimize()\n",
    "    return inst.getAttr(\"X\", vs)\n",
    "\n",
    "\n",
    "def repair(inst, fixed: set, bounds: dict):\n",
    "    old_iis_method = getattr(inst, \"IISMethod\", -1)\n",
    "    inst.setParam(\"IISMethod\", 0)\n",
    "\n",
    "    vs = inst.getVars()\n",
    "    ns = inst.getAttr(\"varName\", vs)\n",
    "    name_to_idx = {n: i for i, n in enumerate(ns)}\n",
    "\n",
    "    freed = set()\n",
    "    while iis_var_names := get_iis_vars(inst):\n",
    "        for n in iis_var_names:\n",
    "            if n not in name_to_idx:\n",
    "                continue\n",
    "\n",
    "            var_idx = name_to_idx[n]\n",
    "            if var_idx not in fixed:\n",
    "                continue\n",
    "\n",
    "            if var_idx in freed:\n",
    "                continue\n",
    "\n",
    "            lb, ub = bounds[var_idx]\n",
    "            vs[var_idx].lb = lb\n",
    "            vs[var_idx].ub = ub\n",
    "            freed.add(var_idx)\n",
    "\n",
    "    inst.setParam(\"IISMethod\", old_iis_method)\n",
    "    return freed\n",
    "\n",
    "\n",
    "def with_lic(m, path=\"gb.lic\"):\n",
    "    with open(path) as f:\n",
    "        env = gp.Env(params=json.load(f))\n",
    "    return m.copy(env=env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from temp.solver.utils import fix_var, repair, set_starts, unfix_var, solve_inst\n",
    "\n",
    "EVIDENCE_FUNCS = {\n",
    "    \"softplus\": (lambda y: F.softplus(y)),\n",
    "    \"relu\": (lambda y: F.relu(y)),\n",
    "    \"exp\": (lambda y: torch.exp(torch.clamp(y, -10, 10))),\n",
    "}\n",
    "\n",
    "\n",
    "def get_predictions(logits, binary_mask):\n",
    "    probs = torch.softmax(logits, axis=1)\n",
    "    preds = probs[:, 1]\n",
    "    probs = _to_numpy(probs)\n",
    "    preds = _to_numpy(preds).squeeze()\n",
    "    preds[binary_mask] = preds[binary_mask].round()\n",
    "    return probs, preds\n",
    "\n",
    "\n",
    "def get_uncertainty(logits, evidence_func_name: str = \"softplus\"):\n",
    "    evidence = EVIDENCE_FUNCS[evidence_func_name](logits)\n",
    "    alpha = evidence + 1\n",
    "    uncertainty = logits.shape[1] / torch.sum(alpha, dim=1, keepdim=True)\n",
    "    return uncertainty\n",
    "\n",
    "\n",
    "def get_threshold(uncertainty: torch.Tensor, r_min: float = 0.4, r_max: float = 0.55):\n",
    "    q = (r_min + r_max) / 2\n",
    "    threshold = torch.quantile(uncertainty, q)\n",
    "    r = (uncertainty <= threshold).float().mean()\n",
    "\n",
    "    if r > r_max:\n",
    "        threshold = torch.quantile(uncertainty, r_max)\n",
    "        return threshold\n",
    "\n",
    "    if r < r_min:\n",
    "        threshold = torch.quantile(uncertainty, r_min)\n",
    "        return threshold\n",
    "\n",
    "    return threshold\n",
    "\n",
    "\n",
    "def get_confident_idx(indices, uncertainty, threshold):\n",
    "    confident_mask = uncertainty <= threshold\n",
    "    confident_idx = indices[confident_mask]\n",
    "    return confident_idx.sort()[0]\n",
    "\n",
    "\n",
    "def solve(inst, prediction, uncertainty, indices, max_iter):\n",
    "    threshold = get_threshold(uncertainty)\n",
    "    conf_idxs = get_confident_idx(indices, uncertainty, threshold)\n",
    "    conf_vals = prediction[conf_idxs]\n",
    "    bounds = fix_var(inst, conf_idxs, conf_vals)\n",
    "    print(len(bounds), len(prediction))\n",
    "\n",
    "    min_q = sum(uncertainty <= threshold) / len(uncertainty)\n",
    "    max_q = 1.0\n",
    "    dq = (max_q - min_q) / (max_iter - 1)\n",
    "\n",
    "    fixed = set(conf_idxs.tolist())\n",
    "    freed = set(repair(inst, fixed, bounds))\n",
    "    for i in range(1, max_iter):\n",
    "        sol = solve_inst(inst)\n",
    "        q = max_q - dq * i\n",
    "        threshold = np.quantile(uncertainty, q)\n",
    "        conf_idxs = get_confident_idx(indices, uncertainty, threshold)\n",
    "        to_unfix = list(fixed - set(conf_idxs.tolist()))\n",
    "        to_unfix = [i for i in to_unfix if i not in freed]\n",
    "        unfix_var(inst, to_unfix, [bounds[i] for i in to_unfix])\n",
    "        starts = {i: sol[i] for i in to_unfix}\n",
    "        starts.update({i: sol[i] for i in freed})\n",
    "        set_starts(inst, starts)\n",
    "    return sol\n",
    "\n",
    "\n",
    "def _to_numpy(tensor_obj):\n",
    "    return tensor_obj.cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter TimeLimit to value 40\n",
      "Set parameter NoRelHeurTime to value 20\n",
      "243 512\n",
      "Set parameter IISMethod to value 0\n",
      "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (mac64[x86] - Darwin 22.4.0 22E252)\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Non-default parameters:\n",
      "TimeLimit  40\n",
      "NoRelHeurTime  20\n",
      "IISMethod  0\n",
      "\n",
      "IIS computation: initial model status unknown, solving to determine model status\n",
      "Found heuristic solution: objective 0.0000000\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.00 seconds (0.00 work units)\n",
      "Thread count was 1 (of 16 available processors)\n",
      "\n",
      "Solution count 1: 0 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 0.000000000000e+00, best bound 0.000000000000e+00, gap 0.0000%\n",
      "IIS runtime: 0.01 seconds (0.00 work units)\n",
      "Cannot compute IIS on a feasible model\n",
      "Set parameter IISMethod to value -1\n",
      "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (mac64[x86] - Darwin 22.4.0 22E252)\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Non-default parameters:\n",
      "TimeLimit  40\n",
      "NoRelHeurTime  20\n",
      "\n",
      "Optimize a model with 512 rows, 512 columns and 26296 nonzeros\n",
      "Model fingerprint: 0xe45de1b5\n",
      "Variable types: 0 continuous, 512 integer (512 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [5e+00, 2e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+00]\n",
      "Found heuristic solution: objective 267.0000000\n",
      "Presolve removed 0 rows and 243 columns\n",
      "Presolve time: 0.01s\n",
      "Presolved: 512 rows, 269 columns, 14379 nonzeros\n",
      "Variable types: 0 continuous, 269 integer (269 binary)\n",
      "Starting NoRel heuristic\n",
      "Found heuristic solution: objective 250.0000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found heuristic solution: objective 241.0000000\n",
      "Found heuristic solution: objective 221.0000000\n",
      "Found heuristic solution: objective 208.0000000\n",
      "Found heuristic solution: objective 189.0000000\n",
      "Found heuristic solution: objective 183.0000000\n",
      "Found heuristic solution: objective 179.0000000\n",
      "Found heuristic solution: objective 177.0000000\n",
      "Found heuristic solution: objective 175.0000000\n",
      "Found heuristic solution: objective 148.0000000\n",
      "Found heuristic solution: objective 143.0000000\n",
      "Found heuristic solution: objective 141.0000000\n",
      "Found heuristic solution: objective 136.0000000\n",
      "Found heuristic solution: objective 134.0000000\n",
      "Found heuristic solution: objective 129.0000000\n",
      "Found heuristic solution: objective 128.0000000\n",
      "Elapsed time for NoRel heuristic: 6s (best bound 76.8308)\n",
      "Found heuristic solution: objective 127.0000000\n",
      "Elapsed time for NoRel heuristic: 11s (best bound 76.8308)\n",
      "Elapsed time for NoRel heuristic: 17s (best bound 76.8308)\n",
      "NoRel heuristic complete\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    0.0000000e+00   5.120000e+02   0.000000e+00     21s\n",
      "     833    7.6830809e+01   0.000000e+00   0.000000e+00     21s\n",
      "\n",
      "Root relaxation: objective 7.683081e+01, 833 iterations, 0.10 seconds (0.11 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0   76.83081    0  148  127.00000   76.83081  39.5%     -   21s\n",
      "     0     0   78.13084    0  144  127.00000   78.13084  38.5%     -   21s\n",
      "     0     0   78.16423    0  145  127.00000   78.16423  38.5%     -   21s\n",
      "     0     0   78.65544    0  146  127.00000   78.65544  38.1%     -   21s\n",
      "     0     0   78.65544    0  143  127.00000   78.65544  38.1%     -   21s\n",
      "     0     0   79.19467    0  146  127.00000   79.19467  37.6%     -   22s\n",
      "     0     0   79.19467    0  139  127.00000   79.19467  37.6%     -   22s\n",
      "     0     0   79.57141    0  145  127.00000   79.57141  37.3%     -   22s\n",
      "     0     0   79.61799    0  145  127.00000   79.61799  37.3%     -   22s\n",
      "     0     2   79.61799    0  145  127.00000   79.61799  37.3%     -   23s\n",
      "   414   433   84.93804   43  117  127.00000   79.61799  37.3%  65.3   25s\n",
      "  1670  1576   80.23858   22  142  127.00000   80.23858  36.8%  67.4   30s\n",
      "  4595  3567   92.57705   67  112  127.00000   80.23858  36.8%  82.8   35s\n",
      " 11446  8975   99.34235   46  103  127.00000   82.98508  34.7%  70.8   40s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 2\n",
      "  Zero half: 5\n",
      "\n",
      "Explored 11624 nodes (824096 simplex iterations) in 40.02 seconds (38.85 work units)\n",
      "Thread count was 16 (of 16 available processors)\n",
      "\n",
      "Solution count 10: 127 128 129 ... 177\n",
      "\n",
      "Time limit reached\n",
      "Best objective 1.270000000000e+02, best bound 8.300000000000e+01, gap 34.6457%\n",
      "[] []\n",
      "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (mac64[x86] - Darwin 22.4.0 22E252)\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Non-default parameters:\n",
      "TimeLimit  40\n",
      "NoRelHeurTime  20\n",
      "\n",
      "Optimize a model with 512 rows, 512 columns and 26296 nonzeros\n",
      "Model fingerprint: 0xe45de1b5\n",
      "Variable types: 0 continuous, 512 integer (512 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [5e+00, 2e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+00]\n",
      "Presolved: 512 rows, 269 columns, 14379 nonzeros\n",
      "\n",
      "Continuing optimization...\n",
      "\n",
      " 11624  9456   87.00365   23  133  127.00000   85.77470  32.5%  70.8   40s\n",
      " 15725 13170   98.77426   35  114  127.00000   86.93046  31.6%  72.0   45s\n",
      " 22667 19114  100.26665   48  111  127.00000   88.02200  30.7%  68.2   50s\n",
      " 29809 24722  106.05176   66  145  127.00000   88.58505  30.2%  67.1   65s\n",
      " 29835 24739  125.06451  139  140  127.00000   88.58505  30.2%  67.0   70s\n",
      "H29847 23509                     126.0000000   88.58505  29.7%  67.0   74s\n",
      " 29852 23513   93.02913   43  140  126.00000   88.58505  29.7%  67.0   75s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 24\n",
      "  Zero half: 1\n",
      "\n",
      "Explored 29865 nodes (2003998 simplex iterations) in 40.03 seconds (54.31 work units)\n",
      "Thread count was 16 (of 16 available processors)\n",
      "\n",
      "Solution count 10: 126 127 128 ... 175\n",
      "\n",
      "Time limit reached\n",
      "Best objective 1.260000000000e+02, best bound 8.900000000000e+01, gap 29.3651%\n",
      "[] []\n",
      "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (mac64[x86] - Darwin 22.4.0 22E252)\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Non-default parameters:\n",
      "TimeLimit  40\n",
      "NoRelHeurTime  20\n",
      "\n",
      "Optimize a model with 512 rows, 512 columns and 26296 nonzeros\n",
      "Model fingerprint: 0xe45de1b5\n",
      "Variable types: 0 continuous, 512 integer (512 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [5e+00, 2e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+00]\n",
      "Presolved: 512 rows, 269 columns, 14379 nonzeros\n",
      "\n",
      "Continuing optimization...\n",
      "\n",
      " 29866 23525   88.58505   31  145  126.00000   88.58505  29.7%  67.1   80s\n",
      " 30321 23842   88.58505   43  127  126.00000   88.58505  29.7%  68.2   85s\n",
      " 31975 24912   93.42510   59  124  126.00000   88.58505  29.7%  69.8   90s\n",
      " 37515 28155   98.57995   69  127  126.00000   88.58505  29.7%  71.6   95s\n",
      " 43351 30939     cutoff   78       126.00000   88.58505  29.7%  71.4  100s\n",
      " 50577 35455   96.57929   39  111  126.00000   89.28784  29.1%  70.4  105s\n",
      " 57794 38544   98.65240   59  113  126.00000   90.00720  28.6%  69.9  110s\n",
      " 64241 42548  114.25445   86  104  126.00000   90.50531  28.2%  71.0  115s\n",
      " 72411 46439  104.06391   62  116  126.00000   91.06147  27.7%  70.5  120s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 24\n",
      "  Flow cover: 2\n",
      "  Zero half: 1\n",
      "\n",
      "Explored 73887 nodes (5204677 simplex iterations) in 40.06 seconds (51.38 work units)\n",
      "Thread count was 16 (of 16 available processors)\n",
      "\n",
      "Solution count 10: 126 127 128 ... 175\n",
      "\n",
      "Time limit reached\n",
      "Best objective 1.260000000000e+02, best bound 9.200000000000e+01, gap 26.9841%\n",
      "[] []\n",
      "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (mac64[x86] - Darwin 22.4.0 22E252)\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Non-default parameters:\n",
      "TimeLimit  40\n",
      "NoRelHeurTime  20\n",
      "\n",
      "Optimize a model with 512 rows, 512 columns and 26296 nonzeros\n",
      "Model fingerprint: 0xe45de1b5\n",
      "Variable types: 0 continuous, 512 integer (512 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [5e+00, 2e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+00]\n",
      "Presolved: 512 rows, 269 columns, 14379 nonzeros\n",
      "\n",
      "Continuing optimization...\n",
      "\n",
      " 73887 47354  119.23802   86   96  126.00000   91.17043  27.6%  70.4  121s\n",
      " 79032 50392  106.22216   75  112  126.00000   91.36613  27.5%  71.1  125s\n",
      " 88660 54876  105.82541   72  102  126.00000   91.90691  27.1%  70.1  130s\n",
      " 95534 58630  112.70673   96   97  126.00000   92.07518  26.9%  70.0  135s\n",
      " 99110 58668  107.93693   55  109  126.00000   92.26213  26.8%  70.1  140s\n",
      "H99742 53245                     122.0000000   92.26271  24.4%  70.2  144s\n",
      " 99890 54520  102.53196   57  121  122.00000   92.26271  24.4%  70.3  145s\n",
      " 104989 58772  106.13825   59  106  122.00000   92.40409  24.3%  70.8  150s\n",
      " 111363 63621  111.09302   80  100  122.00000   92.62950  24.1%  71.8  156s\n",
      " 116492 67465   99.04427   60  106  122.00000   92.70556  24.0%  72.3  160s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 24\n",
      "  Flow cover: 2\n",
      "  Zero half: 1\n",
      "\n",
      "Explored 117743 nodes (8508455 simplex iterations) in 40.07 seconds (44.84 work units)\n",
      "Thread count was 16 (of 16 available processors)\n",
      "\n",
      "Solution count 10: 122 126 127 ... 148\n",
      "\n",
      "Time limit reached\n",
      "Best objective 1.220000000000e+02, best bound 9.300000000000e+01, gap 23.7705%\n",
      "[] []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.arange(0, len(unc), dtype=int)\n",
    "unc = unc.squeeze()\n",
    "to_solve = m.copy()\n",
    "to_solve.setParam(\"TimeLimit\", 40)\n",
    "to_solve.setParam(\"NoRelHeurTime\", 20)\n",
    "solve(to_solve, preds, unc, indices, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter TimeLimit to value 150\n",
      "Set parameter NoRelHeurTime to value 60\n",
      "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (mac64[x86] - Darwin 22.4.0 22E252)\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Non-default parameters:\n",
      "TimeLimit  150\n",
      "NoRelHeurTime  60\n",
      "\n",
      "Optimize a model with 512 rows, 512 columns and 26296 nonzeros\n",
      "Model fingerprint: 0xb6f38ccf\n",
      "Variable types: 0 continuous, 512 integer (512 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [5e+00, 2e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+00]\n",
      "Found heuristic solution: objective 300.0000000\n",
      "Presolve time: 0.16s\n",
      "Presolved: 512 rows, 512 columns, 26296 nonzeros\n",
      "Variable types: 0 continuous, 512 integer (512 binary)\n",
      "Starting NoRel heuristic\n",
      "Found heuristic solution: objective 269.0000000\n",
      "Found heuristic solution: objective 264.0000000\n",
      "Found heuristic solution: objective 233.0000000\n",
      "Found heuristic solution: objective 200.0000000\n",
      "Found heuristic solution: objective 194.0000000\n",
      "Found heuristic solution: objective 183.0000000\n",
      "Found heuristic solution: objective 154.0000000\n",
      "Found heuristic solution: objective 153.0000000\n",
      "Found heuristic solution: objective 149.0000000\n",
      "Found heuristic solution: objective 144.0000000\n",
      "Found heuristic solution: objective 138.0000000\n",
      "Found heuristic solution: objective 137.0000000\n",
      "Found heuristic solution: objective 136.0000000\n",
      "Found heuristic solution: objective 135.0000000\n",
      "Found heuristic solution: objective 134.0000000\n",
      "Found heuristic solution: objective 133.0000000\n",
      "Found heuristic solution: objective 131.0000000\n",
      "Found heuristic solution: objective 130.0000000\n",
      "Elapsed time for NoRel heuristic: 5s (best bound 76.7838)\n",
      "Elapsed time for NoRel heuristic: 11s (best bound 76.7838)\n",
      "Elapsed time for NoRel heuristic: 17s (best bound 76.7838)\n",
      "Elapsed time for NoRel heuristic: 24s (best bound 76.7838)\n",
      "Elapsed time for NoRel heuristic: 35s (best bound 76.7838)\n",
      "Found heuristic solution: objective 128.0000000\n",
      "Elapsed time for NoRel heuristic: 42s (best bound 76.7838)\n",
      "Elapsed time for NoRel heuristic: 55s (best bound 76.7838)\n",
      "Elapsed time for NoRel heuristic: 65s (best bound 76.7838)\n",
      "NoRel heuristic complete\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    0.0000000e+00   5.120000e+02   0.000000e+00     65s\n",
      "     797    7.6783843e+01   0.000000e+00   0.000000e+00     65s\n",
      "\n",
      "Root relaxation: objective 7.678384e+01, 797 iterations, 0.10 seconds (0.11 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0   76.78384    0  153  128.00000   76.78384  40.0%     -   64s\n",
      "     0     0   76.90931    0  150  128.00000   76.90931  39.9%     -   65s\n",
      "     0     0   77.06063    0  146  128.00000   77.06063  39.8%     -   65s\n",
      "     0     0   78.34860    0  144  128.00000   78.34860  38.8%     -   65s\n",
      "     0     0   78.34860    0  145  128.00000   78.34860  38.8%     -   65s\n",
      "     0     0   78.74184    0  142  128.00000   78.74184  38.5%     -   66s\n",
      "     0     0   78.78382    0  144  128.00000   78.78382  38.5%     -   66s\n",
      "     0     0   79.27743    0  143  128.00000   79.27743  38.1%     -   67s\n",
      "     0     0   79.27743    0  143  128.00000   79.27743  38.1%     -   67s\n",
      "     0     2   79.27743    0  143  128.00000   79.27743  38.1%     -   73s\n",
      "   277   286   81.35421   27  131  128.00000   79.27743  38.1%  75.4   75s\n",
      "  1167  1173   88.05670   78  125  128.00000   79.27743  38.1%  62.7   80s\n",
      "  2699  2641  122.85847  208  143  128.00000   79.27743  38.1%  73.5   87s\n",
      "  2709  2648  126.31447  232  143  128.00000   79.27743  38.1%  73.2   92s\n",
      "  2906  2789   80.92442   23  143  128.00000   79.27743  38.1%  81.6   95s\n",
      "  4149  3738   97.22398   69  136  128.00000   79.27743  38.1%   117  100s\n",
      "  6304  5069  112.16330  141  107  128.00000   79.27743  38.1%   121  105s\n",
      " 11423  8420  116.15780  177   94  128.00000   82.37863  35.6%  97.5  110s\n",
      " 15822 12312   96.57319   35  137  128.00000   85.52183  33.2%  92.9  115s\n",
      " 21202 17116  124.42704   72   84  128.00000   86.34541  32.5%  85.0  120s\n",
      " 24764 20364   92.38211   17  128  128.00000   86.91333  32.1%  92.2  125s\n",
      " 28161 23577  124.03242  138  104  128.00000   87.29556  31.8%  92.3  130s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 3\n",
      "  Zero half: 3\n",
      "\n",
      "Explored 29919 nodes (2753110 simplex iterations) in 150.05 seconds (175.47 work units)\n",
      "Thread count was 16 (of 16 available processors)\n",
      "\n",
      "Solution count 10: 128 130 131 ... 144\n",
      "\n",
      "Time limit reached\n",
      "Best objective 1.280000000000e+02, best bound 8.800000000000e+01, gap 31.2500%\n"
     ]
    }
   ],
   "source": [
    "gb_m = m.copy()\n",
    "gb_m.setParam(\"TimeLimit\", 150)\n",
    "gb_m.setParam(\"NoRelHeurTime\", 60)\n",
    "gb_m.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn4co",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
